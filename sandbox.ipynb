{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TFG: Alfonso Moure\n",
    "\n",
    "Este fichero contiene un borrador del proyecto TFG de Alfonso Moure, donde se exploran diferentes técnicas para el trabajo con redes neuronales gráficas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instalando los módulos necesarios\n",
    "\n",
    "Como primer paso, se va a proceder a instalar los módulos necesarios para las pruebas:\n",
    "\n",
    "* [numpy](https://numpy.org/), para poder realizar las operaciones necesarias sobre los datos.\n",
    "* [TensorFlow](https://www.tensorflow.org/), como librería principal para ejecutar los distintos algoritmos de aprendizaje computacional a los que será necesario recurrir.\n",
    "* [Spektral](https://graphneural.network/), una cómoda librería y colección de sets de datos que permite trabajar de manera cómoda con redes neuronales gráficas.\n",
    "\n",
    "Una vez instalados, se procede a imporarlos. Aunque se incluye la descripción del fichero requierements.txt con el proyecto, se proceden a importar desde el notebook para aportar una mayor claridad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in /Users/ghostmou/virtualenvs/uoc-tfg-gnn/lib/python3.9/site-packages (1.19.5)\n",
      "Requirement already satisfied: tensorflow in /Users/ghostmou/virtualenvs/uoc-tfg-gnn/lib/python3.9/site-packages (2.7.0)\n",
      "Requirement already satisfied: flatbuffers<3.0,>=1.12 in /Users/ghostmou/virtualenvs/uoc-tfg-gnn/lib/python3.9/site-packages (from tensorflow) (2.0)\n",
      "Requirement already satisfied: keras-preprocessing>=1.1.1 in /Users/ghostmou/virtualenvs/uoc-tfg-gnn/lib/python3.9/site-packages (from tensorflow) (1.1.2)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /Users/ghostmou/virtualenvs/uoc-tfg-gnn/lib/python3.9/site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.21.0 in /Users/ghostmou/virtualenvs/uoc-tfg-gnn/lib/python3.9/site-packages (from tensorflow) (0.22.0)\n",
      "Requirement already satisfied: tensorboard~=2.6 in /Users/ghostmou/virtualenvs/uoc-tfg-gnn/lib/python3.9/site-packages (from tensorflow) (2.7.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /Users/ghostmou/virtualenvs/uoc-tfg-gnn/lib/python3.9/site-packages (from tensorflow) (1.13.3)\n",
      "Requirement already satisfied: keras<2.8,>=2.7.0rc0 in /Users/ghostmou/virtualenvs/uoc-tfg-gnn/lib/python3.9/site-packages (from tensorflow) (2.7.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /Users/ghostmou/virtualenvs/uoc-tfg-gnn/lib/python3.9/site-packages (from tensorflow) (1.1.0)\n",
      "Requirement already satisfied: protobuf>=3.9.2 in /Users/ghostmou/virtualenvs/uoc-tfg-gnn/lib/python3.9/site-packages (from tensorflow) (3.19.1)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /Users/ghostmou/virtualenvs/uoc-tfg-gnn/lib/python3.9/site-packages (from tensorflow) (4.0.0)\n",
      "Requirement already satisfied: numpy>=1.14.5 in /Users/ghostmou/virtualenvs/uoc-tfg-gnn/lib/python3.9/site-packages (from tensorflow) (1.19.5)\n",
      "Requirement already satisfied: tensorflow-estimator<2.8,~=2.7.0rc0 in /Users/ghostmou/virtualenvs/uoc-tfg-gnn/lib/python3.9/site-packages (from tensorflow) (2.7.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.32.0 in /Users/ghostmou/virtualenvs/uoc-tfg-gnn/lib/python3.9/site-packages (from tensorflow) (0.37.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /Users/ghostmou/virtualenvs/uoc-tfg-gnn/lib/python3.9/site-packages (from tensorflow) (1.42.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /Users/ghostmou/virtualenvs/uoc-tfg-gnn/lib/python3.9/site-packages (from tensorflow) (1.16.0)\n",
      "Requirement already satisfied: gast<0.5.0,>=0.2.1 in /Users/ghostmou/virtualenvs/uoc-tfg-gnn/lib/python3.9/site-packages (from tensorflow) (0.4.0)\n",
      "Requirement already satisfied: absl-py>=0.4.0 in /Users/ghostmou/virtualenvs/uoc-tfg-gnn/lib/python3.9/site-packages (from tensorflow) (1.0.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /Users/ghostmou/virtualenvs/uoc-tfg-gnn/lib/python3.9/site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: libclang>=9.0.1 in /Users/ghostmou/virtualenvs/uoc-tfg-gnn/lib/python3.9/site-packages (from tensorflow) (12.0.0)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /Users/ghostmou/virtualenvs/uoc-tfg-gnn/lib/python3.9/site-packages (from tensorflow) (3.3.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in /Users/ghostmou/virtualenvs/uoc-tfg-gnn/lib/python3.9/site-packages (from tensorflow) (3.6.0)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /Users/ghostmou/virtualenvs/uoc-tfg-gnn/lib/python3.9/site-packages (from tensorboard~=2.6->tensorflow) (2.26.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /Users/ghostmou/virtualenvs/uoc-tfg-gnn/lib/python3.9/site-packages (from tensorboard~=2.6->tensorflow) (3.3.6)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /Users/ghostmou/virtualenvs/uoc-tfg-gnn/lib/python3.9/site-packages (from tensorboard~=2.6->tensorflow) (0.6.1)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /Users/ghostmou/virtualenvs/uoc-tfg-gnn/lib/python3.9/site-packages (from tensorboard~=2.6->tensorflow) (57.4.0)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /Users/ghostmou/virtualenvs/uoc-tfg-gnn/lib/python3.9/site-packages (from tensorboard~=2.6->tensorflow) (2.3.3)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /Users/ghostmou/virtualenvs/uoc-tfg-gnn/lib/python3.9/site-packages (from tensorboard~=2.6->tensorflow) (1.8.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /Users/ghostmou/virtualenvs/uoc-tfg-gnn/lib/python3.9/site-packages (from tensorboard~=2.6->tensorflow) (0.4.6)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /Users/ghostmou/virtualenvs/uoc-tfg-gnn/lib/python3.9/site-packages (from tensorboard~=2.6->tensorflow) (2.0.2)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /Users/ghostmou/virtualenvs/uoc-tfg-gnn/lib/python3.9/site-packages (from google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow) (4.2.4)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /Users/ghostmou/virtualenvs/uoc-tfg-gnn/lib/python3.9/site-packages (from google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /Users/ghostmou/virtualenvs/uoc-tfg-gnn/lib/python3.9/site-packages (from google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow) (4.7.2)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /Users/ghostmou/virtualenvs/uoc-tfg-gnn/lib/python3.9/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow) (1.3.0)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in /Users/ghostmou/virtualenvs/uoc-tfg-gnn/lib/python3.9/site-packages (from markdown>=2.6.8->tensorboard~=2.6->tensorflow) (4.8.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/ghostmou/virtualenvs/uoc-tfg-gnn/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/ghostmou/virtualenvs/uoc-tfg-gnn/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow) (2021.10.8)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /Users/ghostmou/virtualenvs/uoc-tfg-gnn/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow) (2.0.7)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/ghostmou/virtualenvs/uoc-tfg-gnn/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow) (1.26.7)\n",
      "Requirement already satisfied: zipp>=0.5 in /Users/ghostmou/virtualenvs/uoc-tfg-gnn/lib/python3.9/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard~=2.6->tensorflow) (3.6.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /Users/ghostmou/virtualenvs/uoc-tfg-gnn/lib/python3.9/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /Users/ghostmou/virtualenvs/uoc-tfg-gnn/lib/python3.9/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow) (3.1.1)\n",
      "Requirement already satisfied: spektral in /Users/ghostmou/virtualenvs/uoc-tfg-gnn/lib/python3.9/site-packages (1.0.8)\n",
      "Requirement already satisfied: tqdm in /Users/ghostmou/virtualenvs/uoc-tfg-gnn/lib/python3.9/site-packages (from spektral) (4.62.3)\n",
      "Requirement already satisfied: scipy in /Users/ghostmou/virtualenvs/uoc-tfg-gnn/lib/python3.9/site-packages (from spektral) (1.7.2)\n",
      "Requirement already satisfied: scikit-learn in /Users/ghostmou/virtualenvs/uoc-tfg-gnn/lib/python3.9/site-packages (from spektral) (1.0.1)\n",
      "Requirement already satisfied: tensorflow>=2.1.0 in /Users/ghostmou/virtualenvs/uoc-tfg-gnn/lib/python3.9/site-packages (from spektral) (2.7.0)\n",
      "Requirement already satisfied: numpy<1.20 in /Users/ghostmou/virtualenvs/uoc-tfg-gnn/lib/python3.9/site-packages (from spektral) (1.19.5)\n",
      "Requirement already satisfied: requests in /Users/ghostmou/virtualenvs/uoc-tfg-gnn/lib/python3.9/site-packages (from spektral) (2.26.0)\n",
      "Requirement already satisfied: networkx in /Users/ghostmou/virtualenvs/uoc-tfg-gnn/lib/python3.9/site-packages (from spektral) (2.6.3)\n",
      "Requirement already satisfied: pandas in /Users/ghostmou/virtualenvs/uoc-tfg-gnn/lib/python3.9/site-packages (from spektral) (1.3.4)\n",
      "Requirement already satisfied: lxml in /Users/ghostmou/virtualenvs/uoc-tfg-gnn/lib/python3.9/site-packages (from spektral) (4.6.4)\n",
      "Requirement already satisfied: joblib in /Users/ghostmou/virtualenvs/uoc-tfg-gnn/lib/python3.9/site-packages (from spektral) (1.1.0)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /Users/ghostmou/virtualenvs/uoc-tfg-gnn/lib/python3.9/site-packages (from tensorflow>=2.1.0->spektral) (3.3.0)\n",
      "Requirement already satisfied: protobuf>=3.9.2 in /Users/ghostmou/virtualenvs/uoc-tfg-gnn/lib/python3.9/site-packages (from tensorflow>=2.1.0->spektral) (3.19.1)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /Users/ghostmou/virtualenvs/uoc-tfg-gnn/lib/python3.9/site-packages (from tensorflow>=2.1.0->spektral) (1.6.3)\n",
      "Requirement already satisfied: keras<2.8,>=2.7.0rc0 in /Users/ghostmou/virtualenvs/uoc-tfg-gnn/lib/python3.9/site-packages (from tensorflow>=2.1.0->spektral) (2.7.0)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.21.0 in /Users/ghostmou/virtualenvs/uoc-tfg-gnn/lib/python3.9/site-packages (from tensorflow>=2.1.0->spektral) (0.22.0)\n",
      "Requirement already satisfied: flatbuffers<3.0,>=1.12 in /Users/ghostmou/virtualenvs/uoc-tfg-gnn/lib/python3.9/site-packages (from tensorflow>=2.1.0->spektral) (2.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in /Users/ghostmou/virtualenvs/uoc-tfg-gnn/lib/python3.9/site-packages (from tensorflow>=2.1.0->spektral) (3.6.0)\n",
      "Requirement already satisfied: tensorboard~=2.6 in /Users/ghostmou/virtualenvs/uoc-tfg-gnn/lib/python3.9/site-packages (from tensorflow>=2.1.0->spektral) (2.7.0)\n",
      "Requirement already satisfied: gast<0.5.0,>=0.2.1 in /Users/ghostmou/virtualenvs/uoc-tfg-gnn/lib/python3.9/site-packages (from tensorflow>=2.1.0->spektral) (0.4.0)\n",
      "Requirement already satisfied: libclang>=9.0.1 in /Users/ghostmou/virtualenvs/uoc-tfg-gnn/lib/python3.9/site-packages (from tensorflow>=2.1.0->spektral) (12.0.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.32.0 in /Users/ghostmou/virtualenvs/uoc-tfg-gnn/lib/python3.9/site-packages (from tensorflow>=2.1.0->spektral) (0.37.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /Users/ghostmou/virtualenvs/uoc-tfg-gnn/lib/python3.9/site-packages (from tensorflow>=2.1.0->spektral) (0.2.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /Users/ghostmou/virtualenvs/uoc-tfg-gnn/lib/python3.9/site-packages (from tensorflow>=2.1.0->spektral) (4.0.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /Users/ghostmou/virtualenvs/uoc-tfg-gnn/lib/python3.9/site-packages (from tensorflow>=2.1.0->spektral) (1.1.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /Users/ghostmou/virtualenvs/uoc-tfg-gnn/lib/python3.9/site-packages (from tensorflow>=2.1.0->spektral) (1.42.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /Users/ghostmou/virtualenvs/uoc-tfg-gnn/lib/python3.9/site-packages (from tensorflow>=2.1.0->spektral) (1.16.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /Users/ghostmou/virtualenvs/uoc-tfg-gnn/lib/python3.9/site-packages (from tensorflow>=2.1.0->spektral) (1.13.3)\n",
      "Requirement already satisfied: keras-preprocessing>=1.1.1 in /Users/ghostmou/virtualenvs/uoc-tfg-gnn/lib/python3.9/site-packages (from tensorflow>=2.1.0->spektral) (1.1.2)\n",
      "Requirement already satisfied: absl-py>=0.4.0 in /Users/ghostmou/virtualenvs/uoc-tfg-gnn/lib/python3.9/site-packages (from tensorflow>=2.1.0->spektral) (1.0.0)\n",
      "Requirement already satisfied: tensorflow-estimator<2.8,~=2.7.0rc0 in /Users/ghostmou/virtualenvs/uoc-tfg-gnn/lib/python3.9/site-packages (from tensorflow>=2.1.0->spektral) (2.7.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /Users/ghostmou/virtualenvs/uoc-tfg-gnn/lib/python3.9/site-packages (from pandas->spektral) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2017.3 in /Users/ghostmou/virtualenvs/uoc-tfg-gnn/lib/python3.9/site-packages (from pandas->spektral) (2021.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/ghostmou/virtualenvs/uoc-tfg-gnn/lib/python3.9/site-packages (from requests->spektral) (3.3)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /Users/ghostmou/virtualenvs/uoc-tfg-gnn/lib/python3.9/site-packages (from requests->spektral) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/ghostmou/virtualenvs/uoc-tfg-gnn/lib/python3.9/site-packages (from requests->spektral) (2021.10.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/ghostmou/virtualenvs/uoc-tfg-gnn/lib/python3.9/site-packages (from requests->spektral) (1.26.7)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Users/ghostmou/virtualenvs/uoc-tfg-gnn/lib/python3.9/site-packages (from scikit-learn->spektral) (3.0.0)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /Users/ghostmou/virtualenvs/uoc-tfg-gnn/lib/python3.9/site-packages (from tensorboard~=2.6->tensorflow>=2.1.0->spektral) (2.3.3)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /Users/ghostmou/virtualenvs/uoc-tfg-gnn/lib/python3.9/site-packages (from tensorboard~=2.6->tensorflow>=2.1.0->spektral) (57.4.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /Users/ghostmou/virtualenvs/uoc-tfg-gnn/lib/python3.9/site-packages (from tensorboard~=2.6->tensorflow>=2.1.0->spektral) (3.3.6)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /Users/ghostmou/virtualenvs/uoc-tfg-gnn/lib/python3.9/site-packages (from tensorboard~=2.6->tensorflow>=2.1.0->spektral) (1.8.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /Users/ghostmou/virtualenvs/uoc-tfg-gnn/lib/python3.9/site-packages (from tensorboard~=2.6->tensorflow>=2.1.0->spektral) (0.4.6)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /Users/ghostmou/virtualenvs/uoc-tfg-gnn/lib/python3.9/site-packages (from tensorboard~=2.6->tensorflow>=2.1.0->spektral) (2.0.2)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /Users/ghostmou/virtualenvs/uoc-tfg-gnn/lib/python3.9/site-packages (from tensorboard~=2.6->tensorflow>=2.1.0->spektral) (0.6.1)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /Users/ghostmou/virtualenvs/uoc-tfg-gnn/lib/python3.9/site-packages (from google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow>=2.1.0->spektral) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /Users/ghostmou/virtualenvs/uoc-tfg-gnn/lib/python3.9/site-packages (from google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow>=2.1.0->spektral) (4.7.2)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /Users/ghostmou/virtualenvs/uoc-tfg-gnn/lib/python3.9/site-packages (from google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow>=2.1.0->spektral) (4.2.4)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /Users/ghostmou/virtualenvs/uoc-tfg-gnn/lib/python3.9/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow>=2.1.0->spektral) (1.3.0)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in /Users/ghostmou/virtualenvs/uoc-tfg-gnn/lib/python3.9/site-packages (from markdown>=2.6.8->tensorboard~=2.6->tensorflow>=2.1.0->spektral) (4.8.2)\n",
      "Requirement already satisfied: zipp>=0.5 in /Users/ghostmou/virtualenvs/uoc-tfg-gnn/lib/python3.9/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard~=2.6->tensorflow>=2.1.0->spektral) (3.6.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /Users/ghostmou/virtualenvs/uoc-tfg-gnn/lib/python3.9/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow>=2.1.0->spektral) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /Users/ghostmou/virtualenvs/uoc-tfg-gnn/lib/python3.9/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow>=2.1.0->spektral) (3.1.1)\n"
     ]
    }
   ],
   "source": [
    "# Install required modules\n",
    "!pip install numpy\n",
    "!pip install tensorflow\n",
    "!pip install spektral"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import downloaded modules\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras # To be able to use Keras more easily from the code\n",
    "import spektral"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Además, se crea un fichero de configuración para Spektral en el directorio raíz del usuario, tal y como se indica en la configuración de dicho módulo. Se crea en la ruta `~/.spektral/config.json` y se guarda el siguiente contenido:\n",
    "\n",
    "```\n",
    "{\n",
    "        \"dataset_folder\": \"/Users/ghostmou/vscode-projects/uoc-tfg-gnn/spektraldata\"\n",
    "}\n",
    "```\n",
    "\n",
    "Esto indica a Spektral que se desea guardar la información descargada para los juegos de datos que van a ser usados en la ruta indicada."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Carga del juego de datos para las pruebas\n",
    "\n",
    "Como se indica en la memoria de proyecto, se hará uso del juego de datos conocido como CORA. Gracias a Spektral, es sencillo descargar este juego de datos y sus distintas estructuras ya preparadas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ghostmou/virtualenvs/uoc-tfg-gnn/lib/python3.9/site-packages/scipy/sparse/_index.py:125: SparseEfficiencyWarning: Changing the sparsity structure of a csr_matrix is expensive. lil_matrix is more efficient.\n",
      "  self._set_arrayXarray(i, j, x)\n"
     ]
    }
   ],
   "source": [
    "# Download CORA dataset and its different members\n",
    "dataset = spektral.datasets.citation.Citation(\n",
    "    'cora', \n",
    "    random_split=False, # split randomly: 20 nodes per class for training, 30 nodes \n",
    "        # per class for validation; or \"Planetoid\" (Yang et al. 2016)\n",
    "    normalize_x=False,  # normalize the features\n",
    "    dtype=np.float32 # numpy data type for the graph data\n",
    "    )\n",
    "dataset.graphs[0]\n",
    "\n",
    "# Also load a list of labels as names, justo to be able to use it\n",
    "label_names = ['Case_Based', 'Genetic_Algorithms', 'Neural_Networks', 'Probabilistic_Methods', 'Reinforcement_Learning', 'Rule_Learning', 'Theory']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con el conjunto de datos descargado, podemos ver que el resumen nos muestra que se ha descargado un grafo con las siguientes características:\n",
    "\n",
    "* 2708 nodos o vértices forman el grafo.\n",
    "* 1433 atributos de nodo.\n",
    "* 0 atributos de relación, es decir, es un nodo cuyas aristas no contienen información.\n",
    "* 7 clases. En base a la definición del juego de datos, sabemos que los vértices se clasifican en dicho número de grupos. Sin embargo, como se explica en la memoria de proyecto, también es posible hacer uso de GNNs para clasificar grafos, por lo que Spektral nos permite trabajar con dos tipos de etiquetas: de nodo y de grafo.\n",
    "\n",
    "Por otro lado, el conjunto de datos recuperado ya viene dividido en tres grupos de muestras:\n",
    "\n",
    "* Muestras de entrenamiento, que aparecen marcadas mediante enmascaramiento con la estructura `mask_tr`.\n",
    "* Muestras de validación, que hacen lo propio mediante `mask_va`.\n",
    "* Muestras de prueba o test, enmascaradas con `mask_te`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples: 140\n",
      "Validation samples: 500\n",
      "Test samples: 1000\n"
     ]
    }
   ],
   "source": [
    "print(f'Training samples: {np.sum(dataset.mask_tr)}')\n",
    "print(f'Validation samples: {np.sum(dataset.mask_va)}')\n",
    "print(f'Test samples: {np.sum(dataset.mask_te)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparación TensorBoard para seguimiento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Antes de proceder con la implementación de los ejemplos, se prepara un entorno basado en TensorBoard para poder visualizar y analizar la ejecución  y sus resultados de manera visual."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import TensorBoard callback to be able to use it in all the code samples\n",
    "from keras.callbacks import TensorBoard\n",
    "\n",
    "# Prepare placement for the logs\n",
    "import os\n",
    "root_logdir = os.path.join(os.curdir, 'my_logs')\n",
    "\n",
    "def get_run_logdir(model_in_use):\n",
    "    import time\n",
    "    run_id = time.strftime(f'run_{model_in_use}_%Y_%m_%d-%H_%M_%S')\n",
    "    return os.path.join(root_logdir, run_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementación de pruebas\n",
    "\n",
    "A continuación, se va a trabajar en la implementación de varios modelos diferentes de ConvGNN para explicar su funcionamiento:\n",
    "\n",
    "* ConvGNN espectral, con un ejemplo de clasificación de nodos.\n",
    "* ConvGNN espacial con paso de mensajes o MPNN, con un ejemplo de clasificación de nodos.\n",
    "\n",
    "Cada apartado irá rematado con un estudio de precisión. Todos los casos serán ejecutados mediante GridSearch para intentar encontrar el resultado más óptimo bajo las condiciones actuales. Además, se extraerá información gráfica mediante TensorBoard."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementación 1: ConvGNN espectral\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Puede verse que se cuenta con 140 muestras de entrenamiento, 500 de validación y 1000 de test.\n",
    "\n",
    "Dado que este enmascaramiento se lleva a cabo mediante datos binarios (verdadero o falso; incluido o excluido de cada subconjunto, respectivamente), es preciso transformar estos valores en pesos que puedan ser usados durante el proceso de aprendizaje: sabemos que las CNN espestrales no hacen uso de los features de nodos y vértices para su toma de decisiones.\n",
    "\n",
    "Para ello, se crea una función que convierte estas colecciones en una media de peso de los valores de los nodos.\n",
    "\n",
    "**ESTE BLOQUE REQUIERE REVISIÓN DESDE MIS NOTAS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "weighed_mask = [\n",
    "    mask.astype(np.float32) / np.count_nonzero(mask)\n",
    "    for mask in (dataset.mask_tr, dataset.mask_va, dataset.mask_te)\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Así, cada vértice de cada colección de muestras tendrá asignado un peso igual al del resto de su conjunto:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples weight: 0.0071428571827709675\n",
      "Validation samples weight: 0.0020000000949949026\n",
      "Test samples weight: 0.0010000001639127731\n"
     ]
    }
   ],
   "source": [
    "print(f'Training samples weight: {np.nanmean(np.where(weighed_mask[0] > 0, weighed_mask[0],np.nan), 0)}')\n",
    "print(f'Validation samples weight: {np.nanmean(np.where(weighed_mask[1] > 0, weighed_mask[1],np.nan), 0)}')\n",
    "print(f'Test samples weight: {np.nanmean(np.where(weighed_mask[2] > 0, weighed_mask[2],np.nan), 0)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preparados los datos, se puede proceder a definir el modelo. Para ello, se hará uso del existente GCN procedente de Spektral (`spektral.models.gcn`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-05 19:45:44.339440: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# Import model\n",
    "from spektral.models.gcn import GCN\n",
    "\n",
    "# Import optimizers\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Import loss function from Keras\n",
    "from tensorflow.keras.losses import CategoricalCrossentropy\n",
    "\n",
    "# Set initial configuration\n",
    "learning_rate = 0.01\n",
    "reduction_function = 'sum'\n",
    "\n",
    "# Create model from Spektral\n",
    "model_gcn = GCN(n_labels=dataset.n_labels, n_input_channels=dataset.n_node_features)\n",
    "\n",
    "# Compile loaded model\n",
    "model_gcn.compile(\n",
    "    optimizer=Adam(learning_rate=learning_rate), # Set optimizer as Adam with a learning rate of 0.01\n",
    "    loss=CategoricalCrossentropy(reduction=reduction_function), # Loss function to be used.\n",
    "    weighted_metrics=['acc'] # Metrics to be evaluated and weighted during training (Keras doc: https://keras.io/api/models/model_training_apis/)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora que tenemos el modelo creado y compilado, es posible proceder con su entrenamiento. Para ello, se empieza por crear los cargadores (*Loaders*) de Spektral para entrenamiento y validación, básicos para las tareas de entrenamiento. Dentro de Spektral, los *loaders* son usados para generar lotes de subgrafos para poder hacer sucesivas pasadas de entrenamiento en la red convolucional en uso. \n",
    "\n",
    "Gracias a que Spektral está diseñado para trabajar con Keras, la clase `Loader` incluye un método `load` que puede ser usado en para llamar al método `fit` del modelo.\n",
    "\n",
    "Puesto que el conjunto de datos incluye un solo grafo, es posible hacer uso del cargador `SingleLoader`, diseñado para trabajar con este tipo de estructuras de grafo único. Puede ser configurado mediante los siguientes parámetros pasados durante su inicialización:\n",
    "\n",
    "* `dataset`, que incluye, como puede deducirse, la estructura de datos.\n",
    "* `epochs`, con la cantidad de **epochs** que van a ser usados durante la fase de entrenamiento.\n",
    "* `shuffle`, para indicar si se desea barajar los contenidos del conjunto de datos tras cada epoch.\n",
    "* `sample_weights`, que podrá ser usado para indicar el peso de cada observación y que, en caso de ser usado en la inicialización, será devuelto en cada paso de entrenamiento. Esta es la estructura que hemos generado con anterioridad para cada subconjunto de entrenamiento, pruebas y validación en base a las estructuras binarias originales. Así, se usará un vector de pesos diferente según el cargador que estemos preparando.\n",
    "\n",
    "Puesto que el siguiente paso es llevar a cabo el entrenamiento del modelo, se prepararán los cargadores de los datos de entrenamiento y validación mediante `SingleLoader`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spektral.data.loaders import SingleLoader\n",
    "loader_training = SingleLoader(dataset, sample_weights=weighed_mask[0])\n",
    "loader_validation = SingleLoader(dataset, sample_weights=weighed_mask[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con los cargadores listos, es posible proceder a entrenar el modelo mediante la función `fit`. A modo de prueba, se hará con un total de `50` epochs. Además, se incorpora un callback de Keras, `EarlyStopping`, que permitirá detener el entrenamiento cuando el proceso detecte que no se está obteniendo una mejora sustancial.\n",
    "\n",
    "Como se mencionó más arriba, se incorpora también un callback `TensorBoard` para poder analizar las pruebas. Sus resultados serán accesibles mediante el siguiente comando de consola:\n",
    "\n",
    "```shell\n",
    "tensorboard --logdir=./my_logs --port=6006\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "1/1 [==============================] - 1s 1s/step - loss: 5.9384 - acc: 0.0786 - val_loss: 2.2536 - val_acc: 0.2820\n",
      "Epoch 2/50\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 2.8648 - acc: 0.2786 - val_loss: 1.7124 - val_acc: 0.3200\n",
      "Epoch 3/50\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 1.6937 - acc: 0.3000 - val_loss: 1.5578 - val_acc: 0.4300\n",
      "Epoch 4/50\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 1.5336 - acc: 0.5000 - val_loss: 1.5080 - val_acc: 0.4900\n",
      "Epoch 5/50\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 1.2681 - acc: 0.4786 - val_loss: 1.5888 - val_acc: 0.4680\n",
      "Epoch 6/50\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 1.5334 - acc: 0.4500 - val_loss: 1.5364 - val_acc: 0.4940\n",
      "Epoch 7/50\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 1.2792 - acc: 0.5429 - val_loss: 1.5012 - val_acc: 0.5160\n",
      "Epoch 8/50\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 1.1374 - acc: 0.5929 - val_loss: 1.5332 - val_acc: 0.5420\n",
      "Epoch 9/50\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 1.1459 - acc: 0.5857 - val_loss: 1.4869 - val_acc: 0.5560\n",
      "Epoch 10/50\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 1.0695 - acc: 0.6071 - val_loss: 1.4487 - val_acc: 0.5720\n",
      "Epoch 11/50\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 1.0927 - acc: 0.6143 - val_loss: 1.4257 - val_acc: 0.5820\n",
      "Epoch 12/50\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.9524 - acc: 0.6643 - val_loss: 1.4094 - val_acc: 0.5960\n",
      "Epoch 13/50\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.9235 - acc: 0.6429 - val_loss: 1.4050 - val_acc: 0.6060\n",
      "Epoch 14/50\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 1.0009 - acc: 0.6214 - val_loss: 1.4098 - val_acc: 0.6180\n",
      "Epoch 15/50\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.8865 - acc: 0.6643 - val_loss: 1.4273 - val_acc: 0.6160\n",
      "Epoch 16/50\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.8874 - acc: 0.6929 - val_loss: 1.4597 - val_acc: 0.6260\n",
      "Epoch 17/50\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.8366 - acc: 0.6929 - val_loss: 1.4987 - val_acc: 0.6280\n",
      "Epoch 18/50\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.6971 - acc: 0.7143 - val_loss: 1.5442 - val_acc: 0.6500\n",
      "Epoch 19/50\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.8121 - acc: 0.7000 - val_loss: 1.5895 - val_acc: 0.6760\n",
      "Epoch 20/50\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.7935 - acc: 0.7286 - val_loss: 1.6399 - val_acc: 0.6660\n",
      "Epoch 21/50\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.5980 - acc: 0.8000 - val_loss: 1.6863 - val_acc: 0.6880\n",
      "Epoch 22/50\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.5976 - acc: 0.8000 - val_loss: 1.7349 - val_acc: 0.6880\n",
      "Epoch 23/50\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.8458 - acc: 0.7000 - val_loss: 1.7587 - val_acc: 0.6940\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x146c0c580>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit the model\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "model_gcn.fit(\n",
    "    loader_training.load(),\n",
    "    steps_per_epoch=loader_training.steps_per_epoch,\n",
    "    validation_data=loader_validation.load(),\n",
    "    validation_steps=loader_validation.steps_per_epoch,\n",
    "    epochs=50,\n",
    "    callbacks=[\n",
    "        EarlyStopping(patience=10,  restore_best_weights=True), # Early stopping callback\n",
    "        TensorBoard(get_run_logdir('gcn_spectral'))\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez hecho este entrenamiento, se puede proceder a evaluar su eficacia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 11ms/step - loss: 1.3139 - acc: 0.6270\n",
      "Loss: 1.313869833946228\n",
      "Accuracy: 0.6270002722740173\n"
     ]
    }
   ],
   "source": [
    "loader_test = SingleLoader(dataset, sample_weights=weighed_mask[2])\n",
    "results = model_gcn.evaluate(\n",
    "    loader_test.load(), \n",
    "    steps=loader_test.steps_per_epoch\n",
    ")\n",
    "print(f'Loss: {results[0]}')\n",
    "print(f'Accuracy: {results[1]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementación 2: ConvGNN espacial mediante paso de mensajes: MPNN\n",
    "\n",
    "En el ejemplo anterior se ha basado el aprendizaje y la clasificación en la estructura espectral de los datos cargados, es decir, en la estructura de la matriz de adyacencia. Sin embargo, tal y como se explica en la memoria de proyecto, esta aproximación puede ser pobre para escenarios donde las relaciones (aristas) entre entidades (vértices) del grafo tienen un significado o importancia diferente, o cuando el contexto de un nodo, construido mediante los atributos de sus vecinos, es importante para la clasificación.\n",
    "\n",
    "Para poder preparar este ejemplo, se hará uso de la clase `MessagePassing` de Spektral, que ofrece una API ya preparada para configurar la función de activación y personalizar el comportamiento para distintos juegos de datos. Así, será preciso personalizar:\n",
    "\n",
    "* Función para la construcción del mensaje pasado entre dos vértices vía la arista que los une. Es conocida como `message` dentro de la API de Spektral.\n",
    "* Selección de la función de agregación de los mensajes pasados desde cada arista a cada nodo: suma, media, etc. Aparece definida como `aggregate`.\n",
    "\n",
    "Puesto que las GNN basadas en paso de mensajes requieren sucesivas iteraciones que permitan propagar los mensajes a niveles cada vez más lejanos, la función `propagate` lo ejecuta y computa los atributos de cada nodo tras pasar los mensajes de todas las aristas del grafo y computar la correspondiente función de agregación.\n",
    "\n",
    "Puesto que Spektral está construido sobre Keras, es preciso, además, implementar algunos de los métodos heredados de la clase `Layer` para definir la capa gráfica espacial:\n",
    "\n",
    "* `__init__`, donde se define la función de activación, se pasan el resto de hiperparámetros estándar y se guarda el dato del tamaño de la salida, que será usado más adelante.\n",
    "* `build`, cuya misión es la de inicializar los pesos de la capa mediante la llamada a `add_weight` y asignar el tamaño que tendrá la matriz utilizada para almacenar los pesos dentro de la capa. La matriz de pesos tendrá tantas filas como la entrada y columnas como la salida.\n",
    "* `call` es el método encargado de ejecutar los cálculos de la capa. Puesto que estamos hablando de paso de mensajes, la salida de la capa será función de las característica de los nodos de entrada y los pesos de la capa. Al final de la misma, en lugar de llamar a la función de activación, se llama a `propagate`, un método disponible dentro de `MessagePassing` encargado de realizar la propagación de los mensajes de cada nodo a sus vecinos.\n",
    "\n",
    "A estos métodos estándar, se incorporan otros de la propia API de Spektral, especializados en el trabajo con redes gráficas:\n",
    "\n",
    "* `message`, encargado de componer el mensaje que será pasado entre los nodos. En este ejemplo se hace uso de la información de los nodos vecinos, accesibles mediante `get_j`.\n",
    "* `aggregate`, con el cometido de definir la función de agregación para los mensajes del vecindario. Aquí se hace uso de la media de todos los mensajes del vecindario, aunque una posible mejora del modelo podría ser el uso de hiperparámetros para encontrar la que mejores reusltados pueda dar. Esta opción es posible desde el propio constructor.\n",
    "* `update` donde, ahora sí, se hace uso de la función de activaciónn apra actualizar la matriz de pesos.\n",
    "\n",
    "Con todo, se empieza creando la clase MPNNLayer como herencia `MessagePassing` para preparar la personalización de los citados métodos y definir la capa de paso de mensajes del modelo a usar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spektral.layers import MessagePassing\n",
    "\n",
    "class MPNNLayer(MessagePassing):\n",
    "    def __init__(self, n_out, activation, **kwargs):\n",
    "        # Initialize message passing layer with the activation function chosen when creating the model\n",
    "        super().__init__(activation=activation, **kwargs)\n",
    "        self.n_out = n_out\n",
    "\n",
    "    def build(self, batch_input_shape):\n",
    "        self.kernel = self.add_weight(\n",
    "            shape=(batch_input_shape[0][-1], self.n_out)\n",
    "        )\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x, a = inputs\n",
    "\n",
    "        # Update node features based on inputs by multiplying node attributes by \n",
    "        # the weights stored during build.\n",
    "        # Kipf 2016\n",
    "        x = tf.matmul(x, self.kernel)\n",
    "\n",
    "        # Return propagation result\n",
    "        return self.propagate(x=x, a=a)\n",
    "\n",
    "    def message(self, x):\n",
    "        return self.get_j(x)\n",
    "    \n",
    "    def aggregate(self, messages): \n",
    "        # We need to return the result of applying the aggregate function over the messages\n",
    "\n",
    "        # Try to use the mean as aggregate function. We use a scatter mean method for this experiment:\n",
    "        return spektral.layers.ops.scatter_mean(messages, self.index_i, self.n_nodes)\n",
    "\n",
    "    def update(self, embeddings):\n",
    "        return self.activation(embeddings)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creada la capa, podemos hacer uso de la misma creando un modelo mediante la API funcional de Keras. Para ello, se empieza por configurar algunos parámetros básicos para su funcionamiento:\n",
    "\n",
    "* Ratio de regularización (o *regularization rate*): con el fin de regularizar los contenidos. Se inicia con un valor de `5e-6`.\n",
    "* Ratio de aprendizaje (o *learning rate*) con un valor de 0.2.\n",
    "* Número de epochs de entrenamiento, que son fijadas en 20.\n",
    "* Nivel de paciencia para el early stopping que será usado más abajo en la etapa de entrenamiento.\n",
    "\n",
    "Hecho esto, se extran las características principales de los datos: número de nodos, número de careacterísticas por nodo y número de clases para clasificarlos. Estos datos serán usados para instanciar la capa de paso de mensajes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial setup for the model\n",
    "l2_regularization_rate = 5e-6\n",
    "learning_rate = 0.2\n",
    "epochs = 20\n",
    "patience = 10\n",
    "\n",
    "# Input parameters\n",
    "number_of_nodes = dataset.n_nodes\n",
    "number_of_node_features = dataset.n_node_features\n",
    "number_of_labels = dataset.n_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora es el momento de definir las entradas. Para ello, se crearán dos tensores:\n",
    "\n",
    "* `adjacency_matrix` con la matriz de adyacencia, que contendrá la matriz de adyacencia y que permitirá saber las conexiones de cada nodo.\n",
    "* `x_input` con las características de cada nodo.\n",
    "\n",
    "Ambas son usadas para definir la capa de salida del modelo, que será construida mediante la clase definida para el MPNNLayer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define input tensors\n",
    "# We define two input tensors: one for the nodes and one for the adjacency matrix\n",
    "from keras.layers import Input\n",
    "adjacency_input = Input(\n",
    "    (number_of_nodes,), sparse=True, dtype=dataset[0].a.dtype,\n",
    "    name='adjacency_matrix_input'\n",
    ")\n",
    "x_input = Input(\n",
    "    shape=(number_of_node_features,),\n",
    "    name='nodes_input'\n",
    ")\n",
    "\n",
    "# Define output layer based on the MPNN layer defined previously as MPNNLayer\n",
    "mpnn_output_layer = MPNNLayer(\n",
    "    number_of_labels, activation=keras.activations.softmax,\n",
    "    kernel_regularizer=keras.regularizers.l2(l2_regularization_rate),\n",
    "    use_bias=False, name='mpnn_layer'\n",
    ")([x_input, adjacency_input])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mediante una herencia sonbre `keras.models.Model` se puede definir un modelo que tendrá:\n",
    "\n",
    "* Tendrá dos entradas formadas por las dos variables definidas con anterioridad: los datos de cada nodo en `x_input` y la matriz de adyacencia que los relaciona entre sí en `adjacency_matrix`.\n",
    "* La salida será la capa creada en el paso anterior, que ha sido denominada como `mpnn_output`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " nodes_input (InputLayer)       [(None, 1433)]       0           []                               \n",
      "                                                                                                  \n",
      " adjacency_matrix_input (InputL  [(None, 2708)]      0           []                               \n",
      " ayer)                                                                                            \n",
      "                                                                                                  \n",
      " mpnn_layer (MPNNLayer)         (None, 7)            10031       ['nodes_input[0][0]',            \n",
      "                                                                  'adjacency_matrix_input[0][0]'] \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 10,031\n",
      "Trainable params: 10,031\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Build and compile model\n",
    "from keras.models import Model\n",
    "model_mpnn = Model(\n",
    "    inputs=[x_input, adjacency_input],\n",
    "    outputs=mpnn_output_layer\n",
    ")\n",
    "model_mpnn.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=learning_rate), \n",
    "    loss=keras.losses.CategoricalCrossentropy(),\n",
    "    weighted_metrics=['acc']\n",
    ")\n",
    "model_mpnn.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En el resumen impreso del modelo pueden verse las dos capas de entrada descritas, que sirven como entrada para la capa de paso de mensajes. A ellas se conecta la capa de paso de mensajes.\n",
    "\n",
    "Construido el modelo, puede ser entrenado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1/1 [==============================] - 1s 575ms/step - loss: 0.1003 - acc: 0.1786 - val_loss: 0.2189 - val_acc: 0.6420\n",
      "Epoch 2/20\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.0149 - acc: 0.9857 - val_loss: 0.1651 - val_acc: 0.7020\n",
      "Epoch 3/20\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.0031 - acc: 0.9929 - val_loss: 0.1625 - val_acc: 0.7180\n",
      "Epoch 4/20\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.0010 - acc: 1.0000 - val_loss: 0.1714 - val_acc: 0.7180\n",
      "Epoch 5/20\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 4.4914e-04 - acc: 1.0000 - val_loss: 0.1829 - val_acc: 0.7140\n",
      "Epoch 6/20\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 2.2372e-04 - acc: 1.0000 - val_loss: 0.1948 - val_acc: 0.7200\n",
      "Epoch 7/20\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 1.2248e-04 - acc: 1.0000 - val_loss: 0.2062 - val_acc: 0.7160\n",
      "Epoch 8/20\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 7.1937e-05 - acc: 1.0000 - val_loss: 0.2169 - val_acc: 0.7160\n",
      "Epoch 9/20\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 4.4728e-05 - acc: 1.0000 - val_loss: 0.2268 - val_acc: 0.7160\n",
      "Epoch 10/20\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 2.9187e-05 - acc: 1.0000 - val_loss: 0.2359 - val_acc: 0.7160\n",
      "Epoch 11/20\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 1.9862e-05 - acc: 1.0000 - val_loss: 0.2442 - val_acc: 0.7180\n",
      "Epoch 12/20\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 1.4026e-05 - acc: 1.0000 - val_loss: 0.2519 - val_acc: 0.7160\n",
      "Epoch 13/20\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 1.0235e-05 - acc: 1.0000 - val_loss: 0.2588 - val_acc: 0.7160\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x146acb820>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the model\n",
    "loader_tr = SingleLoader(dataset, sample_weights=dataset.mask_tr)\n",
    "loader_va = SingleLoader(dataset, sample_weights=dataset.mask_va)\n",
    "model_mpnn.fit(\n",
    "    loader_tr.load(),\n",
    "    steps_per_epoch=loader_tr.steps_per_epoch,\n",
    "    validation_data=loader_va.load(),\n",
    "    validation_steps=loader_va.steps_per_epoch,\n",
    "    epochs=epochs,\n",
    "    callbacks=[\n",
    "        EarlyStopping(patience=patience, restore_best_weights=True),\n",
    "        TensorBoard(get_run_logdir('gcn_spatial'))\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entrenado, podemos evaluar el modelo frente al conjunto de test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating model.\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.3273 - acc: 0.7320\n",
      "Done.\n",
      "Test loss: 0.3273445665836334\n",
      "Test accuracy: 0.7319999933242798\n"
     ]
    }
   ],
   "source": [
    "# Evaluate model\n",
    "print(\"Evaluating model.\")\n",
    "loader_te = SingleLoader(dataset, sample_weights=dataset.mask_te)\n",
    "eval_results = model_mpnn.evaluate(loader_te.load(), steps=loader_te.steps_per_epoch)\n",
    "print(\"Done.\\n\" \"Test loss: {}\\n\" \"Test accuracy: {}\".format(*eval_results))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementación 3: clasificación de nodos mediante CNN sin uso de estructura gráfica\n",
    "\n",
    "El primer paso será construir una estructura de datos que nos permita trabajar con un modelo que no esté diseñado para operar sobre grafos. Para ello, se extraerán las características de cada observación de los datos de origen y no se usarán sus enlaces. Con todo, el objetivo es medir el nivel de precisión a la hora de clasificar sin utilizar la estructura generado mediante las relaciones entre nodos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: X (140, 1433), y (140, 7), from a source of 140 samples\n",
      "Validation: X (500, 1433), y (500, 7), from a source of 500 samples\n",
      "Test: X (1000, 1433), y (1000, 7), from a source of 1000 samples\n"
     ]
    }
   ],
   "source": [
    "# Extract train, validation and test features and labels\n",
    "X_train = dataset[0].x[np.array(dataset.mask_tr)]\n",
    "y_train = dataset[0].y[np.array(dataset.mask_tr)]\n",
    "X_validation = dataset[0].x[np.array(dataset.mask_va)]\n",
    "y_validation = dataset[0].y[np.array(dataset.mask_va)]\n",
    "X_test = dataset[0].x[np.array(dataset.mask_te)]\n",
    "y_test = dataset[0].y[np.array(dataset.mask_te)]\n",
    "\n",
    "print(f'Training: X {X_train.shape}, y {y_train.shape}, from a source of {np.sum(dataset.mask_tr)} samples')\n",
    "print(f'Validation: X {X_validation.shape}, y {y_validation.shape}, from a source of {np.sum(dataset.mask_va)} samples')\n",
    "print(f'Test: X {X_test.shape}, y {y_test.shape}, from a source of {np.sum(dataset.mask_te)} samples')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "5/5 [==============================] - 0s 35ms/step - loss: 231.7274 - acc: 0.3143 - val_loss: 248.2600 - val_acc: 0.3580\n",
      "Epoch 2/30\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 148.4955 - acc: 0.6357 - val_loss: 254.9398 - val_acc: 0.3600\n",
      "Epoch 3/30\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 144.1733 - acc: 0.7071 - val_loss: nan - val_acc: 0.3640\n",
      "Epoch 4/30\n",
      "5/5 [==============================] - 0s 13ms/step - loss: nan - acc: 0.6500 - val_loss: nan - val_acc: 0.1220\n",
      "Epoch 5/30\n",
      "5/5 [==============================] - 0s 13ms/step - loss: nan - acc: 0.1429 - val_loss: nan - val_acc: 0.1220\n",
      "Epoch 6/30\n",
      "5/5 [==============================] - 0s 14ms/step - loss: nan - acc: 0.1429 - val_loss: nan - val_acc: 0.1220\n",
      "Epoch 7/30\n",
      "5/5 [==============================] - 0s 14ms/step - loss: nan - acc: 0.1429 - val_loss: nan - val_acc: 0.1220\n",
      "Epoch 8/30\n",
      "5/5 [==============================] - 0s 12ms/step - loss: nan - acc: 0.1429 - val_loss: nan - val_acc: 0.1220\n",
      "Epoch 9/30\n",
      "5/5 [==============================] - 0s 12ms/step - loss: nan - acc: 0.1429 - val_loss: nan - val_acc: 0.1220\n",
      "Epoch 10/30\n",
      "5/5 [==============================] - 0s 12ms/step - loss: nan - acc: 0.1429 - val_loss: nan - val_acc: 0.1220\n",
      "Epoch 11/30\n",
      "5/5 [==============================] - 0s 12ms/step - loss: nan - acc: 0.1429 - val_loss: nan - val_acc: 0.1220\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x14741fca0>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=X_train[0].shape),\n",
    "    keras.layers.Dense(300, activation=keras.activations.relu),\n",
    "    keras.layers.Dense(7, activation=keras.activations.relu)\n",
    "])\n",
    "\n",
    "# Compile model (based on the same parameters used with the espectral ConvGNN)\n",
    "model.compile(\n",
    "    optimizer=Adam(learning_rate=0.01), # Set optimizer as Adam with a learning rate of 0.01\n",
    "    loss=CategoricalCrossentropy(reduction=reduction_function), # Loss function to be used.\n",
    "    weighted_metrics=['acc'] # Metrics to be evaluated and weighted during training (Keras doc: https://keras.io/api/models/model_training_apis/)\n",
    ")\n",
    "\n",
    "model.fit(\n",
    "    X_train, y_train, epochs=30, \n",
    "    validation_data=(X_validation, y_validation),\n",
    "    callbacks=[\n",
    "        EarlyStopping(patience=10,  restore_best_weights=True), # Early stopping callback\n",
    "        TensorBoard(get_run_logdir('non_gnn'))\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 0s 1ms/step - loss: 239.7379 - acc: 0.3700\n",
      "Loss: 239.73793029785156\n",
      "Accuracy: 0.3700000047683716\n"
     ]
    }
   ],
   "source": [
    "results = model.evaluate(X_test, y_test)\n",
    "print(f'Loss: {results[0]}')\n",
    "print(f'Accuracy: {results[1]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementación 4: predicción de nodos mediante paso de mensajes\n",
    "\n",
    "En este caso, la implementación será hecha mediante el modelo WalkPooling e implementado mediante la librería Pytorch, según el planteamiento en https://paperswithcode.com/sota/link-prediction-on-cora\n",
    "\n",
    "#TODO Añadir https://paperswithcode.com/sota/link-prediction-on-cora a bibliografía"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone repository from Walkpooling paper https://github.com/DaDaCheng/WalkPooling/\n",
    "# TODO put here the reference from the memory\n",
    "#%%bash\n",
    "#git clone https://github.com/DaDaCheng/WalkPooling/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /Users/ghostmou/virtualenvs/uoc-tfg-gnn/lib/python3.9/site-packages (1.10.0)\n",
      "Requirement already satisfied: typing-extensions in /Users/ghostmou/virtualenvs/uoc-tfg-gnn/lib/python3.9/site-packages (from torch) (4.0.0)\n",
      "Requirement already satisfied: torch-cluster in /Users/ghostmou/virtualenvs/uoc-tfg-gnn/lib/python3.9/site-packages (1.5.9)\n",
      "Requirement already satisfied: torch-scatter in /Users/ghostmou/virtualenvs/uoc-tfg-gnn/lib/python3.9/site-packages (2.0.9)\n",
      "Requirement already satisfied: torch-sparse in /Users/ghostmou/virtualenvs/uoc-tfg-gnn/lib/python3.9/site-packages (0.6.12)\n",
      "Requirement already satisfied: scipy in /Users/ghostmou/virtualenvs/uoc-tfg-gnn/lib/python3.9/site-packages (from torch-sparse) (1.7.2)\n",
      "Requirement already satisfied: numpy<1.23.0,>=1.16.5 in /Users/ghostmou/virtualenvs/uoc-tfg-gnn/lib/python3.9/site-packages (from scipy->torch-sparse) (1.19.5)\n",
      "Requirement already satisfied: torch-geometric in /Users/ghostmou/virtualenvs/uoc-tfg-gnn/lib/python3.9/site-packages (2.0.2)\n",
      "Requirement already satisfied: pyparsing in /Users/ghostmou/virtualenvs/uoc-tfg-gnn/lib/python3.9/site-packages (from torch-geometric) (3.0.6)\n",
      "Requirement already satisfied: pandas in /Users/ghostmou/virtualenvs/uoc-tfg-gnn/lib/python3.9/site-packages (from torch-geometric) (1.3.4)\n",
      "Requirement already satisfied: rdflib in /Users/ghostmou/virtualenvs/uoc-tfg-gnn/lib/python3.9/site-packages (from torch-geometric) (6.0.2)\n",
      "Requirement already satisfied: networkx in /Users/ghostmou/virtualenvs/uoc-tfg-gnn/lib/python3.9/site-packages (from torch-geometric) (2.6.3)\n",
      "Requirement already satisfied: requests in /Users/ghostmou/virtualenvs/uoc-tfg-gnn/lib/python3.9/site-packages (from torch-geometric) (2.26.0)\n",
      "Requirement already satisfied: scikit-learn in /Users/ghostmou/virtualenvs/uoc-tfg-gnn/lib/python3.9/site-packages (from torch-geometric) (1.0.1)\n",
      "Requirement already satisfied: tqdm in /Users/ghostmou/virtualenvs/uoc-tfg-gnn/lib/python3.9/site-packages (from torch-geometric) (4.62.3)\n",
      "Requirement already satisfied: scipy in /Users/ghostmou/virtualenvs/uoc-tfg-gnn/lib/python3.9/site-packages (from torch-geometric) (1.7.2)\n",
      "Requirement already satisfied: googledrivedownloader in /Users/ghostmou/virtualenvs/uoc-tfg-gnn/lib/python3.9/site-packages (from torch-geometric) (0.4)\n",
      "Requirement already satisfied: jinja2 in /Users/ghostmou/virtualenvs/uoc-tfg-gnn/lib/python3.9/site-packages (from torch-geometric) (3.0.3)\n",
      "Requirement already satisfied: yacs in /Users/ghostmou/virtualenvs/uoc-tfg-gnn/lib/python3.9/site-packages (from torch-geometric) (0.1.8)\n",
      "Requirement already satisfied: PyYAML in /Users/ghostmou/virtualenvs/uoc-tfg-gnn/lib/python3.9/site-packages (from torch-geometric) (6.0)\n",
      "Requirement already satisfied: numpy in /Users/ghostmou/virtualenvs/uoc-tfg-gnn/lib/python3.9/site-packages (from torch-geometric) (1.19.5)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/ghostmou/virtualenvs/uoc-tfg-gnn/lib/python3.9/site-packages (from jinja2->torch-geometric) (2.0.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /Users/ghostmou/virtualenvs/uoc-tfg-gnn/lib/python3.9/site-packages (from pandas->torch-geometric) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2017.3 in /Users/ghostmou/virtualenvs/uoc-tfg-gnn/lib/python3.9/site-packages (from pandas->torch-geometric) (2021.3)\n",
      "Requirement already satisfied: isodate in /Users/ghostmou/virtualenvs/uoc-tfg-gnn/lib/python3.9/site-packages (from rdflib->torch-geometric) (0.6.0)\n",
      "Requirement already satisfied: setuptools in /Users/ghostmou/virtualenvs/uoc-tfg-gnn/lib/python3.9/site-packages (from rdflib->torch-geometric) (57.4.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/ghostmou/virtualenvs/uoc-tfg-gnn/lib/python3.9/site-packages (from requests->torch-geometric) (1.26.7)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /Users/ghostmou/virtualenvs/uoc-tfg-gnn/lib/python3.9/site-packages (from requests->torch-geometric) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/ghostmou/virtualenvs/uoc-tfg-gnn/lib/python3.9/site-packages (from requests->torch-geometric) (2021.10.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/ghostmou/virtualenvs/uoc-tfg-gnn/lib/python3.9/site-packages (from requests->torch-geometric) (3.3)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Users/ghostmou/virtualenvs/uoc-tfg-gnn/lib/python3.9/site-packages (from scikit-learn->torch-geometric) (3.0.0)\n",
      "Requirement already satisfied: joblib>=0.11 in /Users/ghostmou/virtualenvs/uoc-tfg-gnn/lib/python3.9/site-packages (from scikit-learn->torch-geometric) (1.1.0)\n",
      "Requirement already satisfied: six>=1.5 in /Users/ghostmou/virtualenvs/uoc-tfg-gnn/lib/python3.9/site-packages (from python-dateutil>=2.7.3->pandas->torch-geometric) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "# Install necessary modules in current environment\n",
    "necessary_modules = ['torch', 'torch-cluster', 'torch-scatter', 'torch-sparse', 'torch-geometric', 'tqdm']\n",
    "for module in necessary_modules:\n",
    "    if module not in sys.modules:\n",
    "        !pip install {module}\n",
    "\n",
    "# %load WalkPooling/src/model.py\n",
    "import torch\n",
    "from torch.nn import Linear, Parameter,Embedding\n",
    "import torch.nn.functional as F\n",
    "from torch_scatter import scatter_mean, scatter, scatter_add, scatter_max\n",
    "from torch_geometric.nn.conv import MessagePassing\n",
    "import torch.nn as nn\n",
    "from torch_geometric.utils import softmax\n",
    "from torch_geometric.nn import GCNConv \n",
    "import numpy as np\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "\n",
    "class LinkPred(MessagePassing):\n",
    "    def __init__(self, in_channels: int, hidden_channels: int, heads: int = 1,\\\n",
    "                 walk_len: int = 6, drnl: bool = False, z_max: int =100, MSE: bool=True):\n",
    "        super(LinkPred, self).__init__()\n",
    "\n",
    "        self.drnl = drnl\n",
    "        if drnl == True:\n",
    "            self.z_embedding = Embedding(z_max, hidden_channels)\n",
    "\n",
    "        self.conv1 = GCNConv(in_channels, hidden_channels)\n",
    "        self.conv2 = GCNConv(hidden_channels, hidden_channels)\n",
    "\n",
    "        self.wp = WalkPooling(in_channels + hidden_channels*2,\\\n",
    "            hidden_channels, heads, walk_len)\n",
    "\n",
    "        L=walk_len*5+1\n",
    "        self.classifier = MLP(L*heads,MSE=MSE)\n",
    "\n",
    "\n",
    "    def forward(self, x, edge_index, edge_mask, batch, z = None):\n",
    "        \n",
    "        #using drnl\n",
    "        if self.drnl == True:\n",
    "            z_emb = self.z_embedding(z)\n",
    "            if z_emb.ndim == 3:  # in case z has multiple integer labels\n",
    "                z_emb = z_emb.sum(dim=1)\n",
    "            z_emb = z_emb.view(x.size(0),-1)\n",
    "            x = torch.cat((x,z_emb.view(x.size(0),-1)),dim=1)\n",
    "        \n",
    "        #GCN layers\n",
    "        x_out = x\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x_out = torch.cat((x_out,x),dim=1)\n",
    "        x = x.relu()\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x_out = torch.cat((x_out,x),dim=1)\n",
    "\n",
    "        #Walk Pooling\n",
    "        feature_list = self.wp(x_out, edge_index, edge_mask, batch)\n",
    "\n",
    "        #Classifier\n",
    "        out = self.classifier(feature_list)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class WalkPooling(MessagePassing):\n",
    "    def __init__(self, in_channels: int, hidden_channels: int, heads: int = 1,\\\n",
    "                 walk_len: int = 6):\n",
    "        super(WalkPooling, self).__init__()\n",
    "\n",
    "        self.hidden_channels = hidden_channels\n",
    "        self.heads = heads\n",
    "        self.walk_len = walk_len\n",
    "\n",
    "        # the linear layers in the attention encoder\n",
    "        self.lin_key1 = Linear(in_channels, hidden_channels)\n",
    "        self.lin_query1 = Linear(in_channels, hidden_channels)\n",
    "        self.lin_key2 = Linear(hidden_channels, heads * hidden_channels)\n",
    "        self.lin_query2 = Linear(hidden_channels, heads * hidden_channels)\n",
    "    def attention_mlp(self, x, edge_index):\n",
    "    \n",
    "        query = self.lin_key1(x).reshape(-1,self.hidden_channels)\n",
    "        key = self.lin_query1(x).reshape(-1,self.hidden_channels)\n",
    "\n",
    "        query = F.leaky_relu(query,0.2)\n",
    "        key = F.leaky_relu(key,0.2)\n",
    "\n",
    "        query = F.dropout(query, p=0.5, training=self.training)\n",
    "        key = F.dropout(key, p=0.5, training=self.training)\n",
    "\n",
    "        query = self.lin_key2(query).view(-1, self.heads, self.hidden_channels)\n",
    "        key = self.lin_query2(key).view(-1, self.heads, self.hidden_channels)\n",
    "\n",
    "        row, col = edge_index\n",
    "        weights = (query[row] * key[col]).sum(dim=-1) / np.sqrt(self.hidden_channels)\n",
    "        \n",
    "        return weights\n",
    "\n",
    "    def weight_encoder(self, x, edge_index, edge_mask):        \n",
    "     \n",
    "        weights = self.attention_mlp(x, edge_index)\n",
    "    \n",
    "        omega = torch.sigmoid(weights[torch.logical_not(edge_mask)])\n",
    "        \n",
    "        row, col = edge_index\n",
    "        num_nodes = torch.max(edge_index)+1\n",
    "\n",
    "        # edge weights of the plus graph\n",
    "        weights_p = softmax(weights,edge_index[1])\n",
    "\n",
    "        # edge weights of the minus graph\n",
    "        weights_m = weights - scatter_max(weights, col, dim=0, dim_size=num_nodes)[0][col]\n",
    "        weights_m = torch.exp(weights_m)\n",
    "        weights_m = weights_m * edge_mask.view(-1,1)\n",
    "        norm = scatter_add(weights_m, col, dim=0, dim_size=num_nodes)[col] + 1e-16\n",
    "        weights_m = weights_m / norm\n",
    "\n",
    "        return weights_p, weights_m, omega\n",
    "\n",
    "    def forward(self, x, edge_index, edge_mask, batch):\n",
    "        \n",
    "        #encode the node representation into edge weights via attention mechanism\n",
    "        weights_p, weights_m, omega = self.weight_encoder(x, edge_index, edge_mask)\n",
    "\n",
    "        # pytorch geometric set the batch adjacency matrix to\n",
    "        # be the diagonal matrix with each graph's adjacency matrix\n",
    "        # stacked in the diagonal. Therefore, calculating the powers\n",
    "        # of the stochastic matrix directly will cost lots of memory.\n",
    "        # We compute the powers of stochastic matrix as follows\n",
    "        # Let A = diag ([A_1,\\cdots,A_n]) be the batch adjacency matrix,\n",
    "        # we set x = [x_1,\\cdots,x_n]^T be the batch feature matrix\n",
    "        # for the i-th graph in the batch with n_i nodes, its feature \n",
    "        # is a n_i\\times n_max matrix, where n_max is the largest number\n",
    "        # of nodes for all graphs in the batch. The elements of x_i are\n",
    "        # (x_i)_{x,y} = \\delta_{x,y}. \n",
    "\n",
    "        # number of graphs in the batch\n",
    "        batch_size = torch.max(batch)+1\n",
    "\n",
    "        # for node i in the batched graph, index[i] is i's id in the graph before batch \n",
    "        index = torch.zeros(batch.size(0),1,dtype=torch.long)\n",
    "        \n",
    "        # numer of nodes in each graph\n",
    "        _, counts = torch.unique(batch, sorted=True, return_counts=True)\n",
    "        \n",
    "        # maximum number of nodes for all graphs in the batch\n",
    "        max_nodes = torch.max(counts)\n",
    "\n",
    "        # set the values in index\n",
    "        id_start = 0\n",
    "        for i in range(batch_size):\n",
    "            index[id_start:id_start+counts[i]] = torch.arange(0,counts[i],dtype=torch.long).view(-1,1)\n",
    "            id_start = id_start+counts[i]\n",
    "\n",
    "        index = index.to(device)\n",
    "        \n",
    "        #the output graph features of walk pooling\n",
    "        nodelevel_p = torch.zeros(batch_size,(self.walk_len*self.heads)).to(device)\n",
    "        nodelevel_m = torch.zeros(batch_size,(self.walk_len*self.heads)).to(device)\n",
    "        linklevel_p = torch.zeros(batch_size,(self.walk_len*self.heads)).to(device)\n",
    "        linklevel_m = torch.zeros(batch_size,(self.walk_len*self.heads)).to(device)\n",
    "        graphlevel = torch.zeros(batch_size,(self.walk_len*self.heads)).to(device)\n",
    "        # a link (i,j) has two directions i->j and j->i, and\n",
    "        # when extract the features of the link, we usually average over\n",
    "        # the two directions. indices_odd and indices_even records the\n",
    "        # indices for a link in two directions\n",
    "        indices_odd = torch.arange(0,omega.size(0),2).to(device)\n",
    "        indices_even = torch.arange(1,omega.size(0),2).to(device)\n",
    "\n",
    "        omega = torch.index_select(omega, 0 ,indices_even)\\\n",
    "        + torch.index_select(omega,0,indices_odd)\n",
    "        \n",
    "        #node id of the candidate (or perturbation) link\n",
    "        link_ij, link_ji = edge_index[:,torch.logical_not(edge_mask)]\n",
    "        node_i = link_ij[indices_odd]\n",
    "        node_j = link_ij[indices_even]\n",
    "\n",
    "        # compute the powers of stochastic matrix\n",
    "        for head in range(self.heads):\n",
    "\n",
    "            # x on the plus graph and minus graph\n",
    "            x_p = torch.zeros(batch.size(0),max_nodes,dtype=x.dtype).to(device)\n",
    "            x_p = x_p.scatter_(1,index,1)\n",
    "            x_m = torch.zeros(batch.size(0),max_nodes,dtype=x.dtype).to(device)\n",
    "            x_m = x_m.scatter_(1,index,1)\n",
    "\n",
    "            # propagage once\n",
    "            x_p = self.propagate(edge_index, x= x_p, norm = weights_p[:,head])\n",
    "            x_m = self.propagate(edge_index, x= x_m, norm = weights_m[:,head])\n",
    "        \n",
    "            # start from tau = 2\n",
    "            for i in range(self.walk_len):\n",
    "                x_p = self.propagate(edge_index, x= x_p, norm = weights_p[:,head])\n",
    "                x_m = self.propagate(edge_index, x= x_m, norm = weights_m[:,head])\n",
    "                \n",
    "                # returning probabilities around i + j\n",
    "                nodelevel_p_w = x_p[node_i,index[node_i].view(-1)] + x_p[node_j,index[node_j].view(-1)]\n",
    "                nodelevel_m_w = x_m[node_i,index[node_i].view(-1)] + x_m[node_j,index[node_j].view(-1)]\n",
    "                nodelevel_p[:,head*self.walk_len+i] = nodelevel_p_w.view(-1)\n",
    "                nodelevel_m[:,head*self.walk_len+i] = nodelevel_m_w.view(-1)\n",
    "  \n",
    "                # transition probabilities between i and j\n",
    "                linklevel_p_w = x_p[node_i,index[node_j].view(-1)] + x_p[node_j,index[node_i].view(-1)]\n",
    "                linklevel_m_w = x_m[node_i,index[node_j].view(-1)] + x_m[node_j,index[node_i].view(-1)]\n",
    "                linklevel_p[:,head*self.walk_len+i] = linklevel_p_w.view(-1)\n",
    "                linklevel_m[:,head*self.walk_len+i] = linklevel_m_w.view(-1)\n",
    "\n",
    "                # graph average of returning probabilities\n",
    "                diag_ele_p = torch.gather(x_p,1,index)\n",
    "                diag_ele_m = torch.gather(x_m,1,index)\n",
    "\n",
    "                graphlevel_p = scatter_add(diag_ele_p, batch, dim = 0)\n",
    "                graphlevel_m = scatter_add(diag_ele_m, batch, dim = 0)\n",
    "\n",
    "                graphlevel[:,head*self.walk_len+i] = (graphlevel_p-graphlevel_m).view(-1)\n",
    "         \n",
    "        feature_list = graphlevel \n",
    "        feature_list = torch.cat((feature_list,omega),dim=1)\n",
    "        feature_list = torch.cat((feature_list,nodelevel_p),dim=1)\n",
    "        feature_list = torch.cat((feature_list,nodelevel_m),dim=1)\n",
    "        feature_list = torch.cat((feature_list,linklevel_p),dim=1)\n",
    "        feature_list = torch.cat((feature_list,linklevel_m),dim=1)\n",
    "\n",
    "\n",
    "        return feature_list\n",
    "\n",
    "    def message(self, x_j, norm):\n",
    "        return norm.view(-1, 1) * x_j  \n",
    "\n",
    "class MLP(torch.nn.Module):\n",
    "    # adopt a MLP as classifier for graphs\n",
    "    def __init__(self,input_size,MSE=True):\n",
    "        super(MLP, self).__init__()\n",
    "        self.nn = nn.BatchNorm1d(input_size)\n",
    "        self.linear1 = torch.nn.Linear(input_size,input_size*20)\n",
    "        self.linear2 = torch.nn.Linear(input_size*20,input_size*20)\n",
    "        self.linear3 = torch.nn.Linear(input_size*20,input_size*10)\n",
    "        self.linear4 = torch.nn.Linear(input_size*10,input_size)\n",
    "        self.linear5 = torch.nn.Linear(input_size,1)\n",
    "        self.act= nn.ReLU()\n",
    "        self.MSE=MSE\n",
    "    def forward(self, x):\n",
    "        out= self.nn(x)\n",
    "        out= self.linear1(out)\n",
    "        out = self.act(out)\n",
    "        out= self.linear2(out)\n",
    "        out = self.act(out)\n",
    "        out = self.linear3(out)\n",
    "        out = self.act(out)\n",
    "        out = self.linear4(out)\n",
    "        out = self.act(out)\n",
    "        out = F.dropout(out, p=0.5, training=self.training)\n",
    "        out = self.linear5(out)\n",
    "        #out = torch.sigmoid(out)\n",
    "        if self.MSE:\n",
    "            out = torch.sigmoid(out)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load WalkPooling/software/drnl.py\n",
    "\"Code adopted and implemented from https://github.com/muhanzhang/SEAL\"\n",
    "import scipy.sparse as ssp\n",
    "from scipy.sparse.csgraph import shortest_path\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "def drnl_node_labeling(edge_index, src, dst, num_nodes):\n",
    "\n",
    "    edge_weight = torch.ones(edge_index.size(1), dtype=int)\n",
    "    adj = ssp.csr_matrix(\n",
    "            (edge_weight, (edge_index[0], edge_index[1])), \n",
    "            shape=(num_nodes, num_nodes))\n",
    "    # Double Radius Node Labeling (DRNL).\n",
    "    src, dst = (dst, src) if src > dst else (src, dst)\n",
    "\n",
    "    idx = list(range(src)) + list(range(src + 1, adj.shape[0]))\n",
    "    adj_wo_src = adj[idx, :][:, idx]\n",
    "\n",
    "    idx = list(range(dst)) + list(range(dst + 1, adj.shape[0]))\n",
    "    adj_wo_dst = adj[idx, :][:, idx]\n",
    "\n",
    "    dist2src = shortest_path(adj_wo_dst, directed=False, unweighted=True, indices=src)\n",
    "    dist2src = np.insert(dist2src, dst, 0, axis=0)\n",
    "    dist2src = torch.from_numpy(dist2src)\n",
    "\n",
    "    dist2dst = shortest_path(adj_wo_src, directed=False, unweighted=True, indices=dst-1)\n",
    "    dist2dst = np.insert(dist2dst, src, 0, axis=0)\n",
    "    dist2dst = torch.from_numpy(dist2dst)\n",
    "\n",
    "    dist = dist2src + dist2dst\n",
    "    dist_over_2, dist_mod_2 = dist // 2, dist % 2\n",
    "\n",
    "    z = 1 + torch.min(dist2src, dist2dst)\n",
    "    z += dist_over_2 * (dist_over_2 + dist_mod_2 - 1)\n",
    "    z[src] = 1.\n",
    "    z[dst] = 1.\n",
    "    z[torch.isnan(z)] = 0.\n",
    "    return z.to(torch.int)\n",
    "\n",
    "# def drnl_node_labeling(edge_index, src, dst, num_nodes):\n",
    "#     # Distance Encoding Plus. When computing distance to src, temporarily mask dst;\n",
    "#     # when computing distance to dst, temporarily mask src. Essentially the same as DRNL.\n",
    "#     max_dist=100\n",
    "#     edge_weight = torch.ones(edge_index.size(1), dtype=int)\n",
    "#     adj = ssp.csr_matrix(\n",
    "#             (edge_weight, (edge_index[0], edge_index[1])), \n",
    "#             shape=(num_nodes, num_nodes))\n",
    "#     src, dst = (dst, src) if src > dst else (src, dst)\n",
    "\n",
    "#     idx = list(range(src)) + list(range(src + 1, adj.shape[0]))\n",
    "#     adj_wo_src = adj[idx, :][:, idx]\n",
    "\n",
    "#     idx = list(range(dst)) + list(range(dst + 1, adj.shape[0]))\n",
    "#     adj_wo_dst = adj[idx, :][:, idx]\n",
    "\n",
    "#     dist2src = shortest_path(adj_wo_dst, directed=False, unweighted=True, indices=src)\n",
    "#     dist2src = np.insert(dist2src, dst, 0, axis=0)\n",
    "#     dist2src = torch.from_numpy(dist2src)\n",
    "\n",
    "#     dist2dst = shortest_path(adj_wo_src, directed=False, unweighted=True, indices=dst-1)\n",
    "#     dist2dst = np.insert(dist2dst, src, 0, axis=0)\n",
    "#     dist2dst = torch.from_numpy(dist2dst)\n",
    "\n",
    "#     dist = torch.cat([dist2src.view(-1, 1), dist2dst.view(-1, 1)], 1)\n",
    "#     dist[dist > max_dist] = max_dist\n",
    "#     dist[torch.isnan(dist)] = max_dist + 1\n",
    "#     dist = torch.sum(dist,dim=1)\n",
    "\n",
    "#     return dist.to(torch.int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load WalkPooling/src/utils.py\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.loader import DataLoader\n",
    "import torch\n",
    "import argparse\n",
    "import numpy as np\n",
    "import math\n",
    "from torch_geometric.utils import to_undirected, from_scipy_sparse_matrix,dense_to_sparse,is_undirected\n",
    "from torch_geometric.transforms import NormalizeFeatures\n",
    "from torch_geometric.datasets import Planetoid\n",
    "import torch.nn.functional as F\n",
    "import sys\n",
    "import os.path\n",
    "import pickle as pkl\n",
    "import networkx as nx\n",
    "import scipy.sparse as sp\n",
    "\n",
    "\n",
    "cur_dir = os.path.abspath('') #os.path.dirname(os.path.realpath(__file__))\n",
    "par_dir = os.path.abspath(os.path.join(cur_dir, '..'))#os.path.abspath(os.path.join(os.path.dirname(__file__),\"..\"))\n",
    "#par_dir = '/Users/ghostmou/vscode-projects/uoc-tfg-gnn/WalkPooling' #os.path.abspath(os.path.join(os.path.dirname(__file__),\"..\"))\n",
    "sys.path.append('%s/WalkPooling/software/' % cur_dir)\n",
    "from drnl import drnl_node_labeling\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "def floor(x):\n",
    "    return torch.div(x, 1, rounding_mode='trunc')\n",
    "\n",
    "def set_random_seed(seed):\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "def split_edges(data,args):\n",
    "    set_random_seed(args.seed)\n",
    "    row, col = data.edge_index\n",
    "    mask = row < col\n",
    "    row, col = row[mask], col[mask]\n",
    "    n_v= floor(args.val_ratio * row.size(0)).int() #number of validation positive edges\n",
    "    n_t=floor(args.test_ratio * row.size(0)).int() #number of test positive edges\n",
    "    #split positive edges   \n",
    "    perm = torch.randperm(row.size(0))\n",
    "    row, col = row[perm], col[perm]\n",
    "    r, c = row[:n_v], col[:n_v]\n",
    "    data.val_pos = torch.stack([r, c], dim=0)\n",
    "    r, c = row[n_v:n_v+n_t], col[n_v:n_v+n_t]\n",
    "    data.test_pos = torch.stack([r, c], dim=0)\n",
    "    r, c = row[n_v+n_t:], col[n_v+n_t:]\n",
    "    data.train_pos = torch.stack([r, c], dim=0)\n",
    "\n",
    "    #sample negative edges\n",
    "    if args.practical_neg_sample == False:\n",
    "        # If practical_neg_sample == False, the sampled negative edges\n",
    "        # in the training and validation set aware the test set\n",
    "\n",
    "        neg_adj_mask = torch.ones(data.num_nodes, data.num_nodes, dtype=torch.uint8)\n",
    "        neg_adj_mask = neg_adj_mask.triu(diagonal=1).to(torch.bool)\n",
    "        neg_adj_mask[row, col] = 0\n",
    "\n",
    "        # Sample all the negative edges and split into val, test, train negs\n",
    "        neg_row, neg_col = neg_adj_mask.nonzero(as_tuple=False).t()\n",
    "        perm = torch.randperm(neg_row.size(0))[:row.size(0)]\n",
    "        neg_row, neg_col = neg_row[perm], neg_col[perm]\n",
    "\n",
    "        row, col = neg_row[:n_v], neg_col[:n_v]\n",
    "        data.val_neg = torch.stack([row, col], dim=0)\n",
    "\n",
    "        row, col = neg_row[n_v:n_v + n_t], neg_col[n_v:n_v + n_t]\n",
    "        data.test_neg = torch.stack([row, col], dim=0)\n",
    "\n",
    "        row, col = neg_row[n_v + n_t:], neg_col[n_v + n_t:]\n",
    "        data.train_neg = torch.stack([row, col], dim=0)\n",
    "\n",
    "    else:\n",
    "\n",
    "        neg_adj_mask = torch.ones(data.num_nodes, data.num_nodes, dtype=torch.uint8)\n",
    "        neg_adj_mask = neg_adj_mask.triu(diagonal=1).to(torch.bool)\n",
    "        neg_adj_mask[row, col] = 0\n",
    "\n",
    "        # Sample the test negative edges first\n",
    "        neg_row, neg_col = neg_adj_mask.nonzero(as_tuple=False).t()\n",
    "        perm = torch.randperm(neg_row.size(0))[:n_t]\n",
    "        neg_row, neg_col = neg_row[perm], neg_col[perm]\n",
    "        data.test_neg = torch.stack([neg_row, neg_col], dim=0)\n",
    "\n",
    "        # Sample the train and val negative edges with only knowing \n",
    "        # the train positive edges\n",
    "        row, col = data.train_pos\n",
    "        neg_adj_mask = torch.ones(data.num_nodes, data.num_nodes, dtype=torch.uint8)\n",
    "        neg_adj_mask = neg_adj_mask.triu(diagonal=1).to(torch.bool)\n",
    "        neg_adj_mask[row, col] = 0\n",
    "\n",
    "        # Sample the train and validation negative edges\n",
    "        neg_row, neg_col = neg_adj_mask.nonzero(as_tuple=False).t()\n",
    "\n",
    "        n_tot = n_v + data.train_pos.size(1)\n",
    "        perm = torch.randperm(neg_row.size(0))[:n_tot]\n",
    "        neg_row, neg_col = neg_row[perm], neg_col[perm]\n",
    "\n",
    "        row, col = neg_row[:n_v], neg_col[:n_v]\n",
    "        data.val_neg = torch.stack([row, col], dim=0)\n",
    "\n",
    "        row, col = neg_row[n_v:], neg_col[n_v:]\n",
    "        data.train_neg = torch.stack([row, col], dim=0)\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "def k_hop_subgraph(node_idx, num_hops, edge_index, max_nodes_per_hop = None,num_nodes = None):\n",
    "  \n",
    "    if num_nodes == None:\n",
    "        num_nodes = torch.max(edge_index)+1\n",
    "    row, col = edge_index\n",
    "    node_mask = row.new_empty(num_nodes, dtype=torch.bool)\n",
    "    edge_mask = row.new_empty(row.size(0), dtype=torch.bool)\n",
    "\n",
    "    node_idx = node_idx.to(row.device)\n",
    "\n",
    "    subsets = [node_idx]\n",
    "\n",
    "    if max_nodes_per_hop == None:\n",
    "        for _ in range(num_hops):\n",
    "            node_mask.fill_(False)\n",
    "            node_mask[subsets[-1]] = True\n",
    "            torch.index_select(node_mask, 0, row, out = edge_mask)\n",
    "            subsets.append(col[edge_mask])\n",
    "    else:\n",
    "        not_visited = row.new_empty(num_nodes, dtype=torch.bool)\n",
    "        not_visited.fill_(True)\n",
    "        for _ in range(num_hops):\n",
    "            node_mask.fill_(False)# the source node mask in this hop\n",
    "            node_mask[subsets[-1]] = True #mark the sources\n",
    "            not_visited[subsets[-1]] = False # mark visited nodes\n",
    "            torch.index_select(node_mask, 0, row, out = edge_mask) # indices of all neighbors\n",
    "            neighbors = col[edge_mask].unique() #remove repeats\n",
    "            neighbor_mask = row.new_empty(num_nodes, dtype=torch.bool) # mask of all neighbor nodes\n",
    "            edge_mask_hop = row.new_empty(row.size(0), dtype=torch.bool) # selected neighbor mask in this hop\n",
    "            neighbor_mask.fill_(False)\n",
    "            neighbor_mask[neighbors] = True\n",
    "            neighbor_mask = torch.logical_and(neighbor_mask, not_visited) # all neighbors that are not visited\n",
    "            ind = torch.where(neighbor_mask==True) #indicies of all the unvisited neighbors\n",
    "            if ind[0].size(0) > max_nodes_per_hop:\n",
    "                perm = torch.randperm(ind[0].size(0))\n",
    "                ind = ind[0][perm]\n",
    "                neighbor_mask[ind[max_nodes_per_hop:]] = False # randomly select max_nodes_per_hop nodes\n",
    "                torch.index_select(neighbor_mask, 0, col, out = edge_mask_hop)# find the indicies of selected nodes\n",
    "                edge_mask = torch.logical_and(edge_mask,edge_mask_hop) # change edge_mask\n",
    "            subsets.append(col[edge_mask])\n",
    "\n",
    "    subset, inv = torch.cat(subsets).unique(return_inverse=True)\n",
    "    inv = inv[:node_idx.numel()]\n",
    "\n",
    "    node_mask.fill_(False)\n",
    "    node_mask[subset] = True\n",
    "    edge_mask = node_mask[row] & node_mask[col]\n",
    "\n",
    "    edge_index = edge_index[:, edge_mask]\n",
    "\n",
    "    node_idx = row.new_full((num_nodes, ), -1)\n",
    "    node_idx[subset] = torch.arange(subset.size(0), device=row.device)\n",
    "    edge_index = node_idx[edge_index]\n",
    "\n",
    "    return subset, edge_index, inv, edge_mask\n",
    "\n",
    "def plus_edge(data_observed, label, p_edge, args):\n",
    "    nodes, edge_index_m, mapping, _ = k_hop_subgraph(node_idx= p_edge, num_hops=args.num_hops,\\\n",
    " edge_index = data_observed.edge_index, max_nodes_per_hop=args.max_nodes_per_hop ,num_nodes=data_observed.num_nodes)\n",
    "    x_sub = data_observed.x[nodes,:]\n",
    "    edge_index_p = edge_index_m\n",
    "    edge_index_p = torch.cat((edge_index_p, mapping.view(-1,1)),dim=1)\n",
    "    edge_index_p = torch.cat((edge_index_p, mapping[[1,0]].view(-1,1)),dim=1)\n",
    "\n",
    "    #edge_mask marks the edge under perturbation, i.e., the candidate edge for LP\n",
    "    edge_mask = torch.ones(edge_index_p.size(1),dtype=torch.bool)\n",
    "    edge_mask[-1] = False\n",
    "    edge_mask[-2] = False\n",
    "\n",
    "    if args.drnl == True:\n",
    "        num_nodes = torch.max(edge_index_p)+1\n",
    "        z = drnl_node_labeling(edge_index_m, mapping[0],mapping[1],num_nodes)\n",
    "        data = Data(edge_index = edge_index_p, x = x_sub, z = z)\n",
    "    else:\n",
    "        data = Data(edge_index = edge_index_p, x = x_sub, z = 0)\n",
    "    data.edge_mask = edge_mask\n",
    "\n",
    "    #label = 1 if the candidate link (p_edge) is positive and label=0 otherwise\n",
    "    data.label = float(label)\n",
    "\n",
    "    return data\n",
    "\n",
    "def minus_edge(data_observed, label, p_edge, args):\n",
    "    nodes, edge_index_p, mapping,_ = k_hop_subgraph(node_idx= p_edge, num_hops=args.num_hops,\\\n",
    " edge_index = data_observed.edge_index,max_nodes_per_hop=args.max_nodes_per_hop, num_nodes = data_observed.num_nodes)\n",
    "    x_sub = data_observed.x[nodes,:]\n",
    "\n",
    "    #edge_mask marks the edge under perturbation, i.e., the candidate edge for LP\n",
    "    edge_mask = torch.ones(edge_index_p.size(1), dtype = torch.bool)\n",
    "    ind = torch.where((edge_index_p == mapping.view(-1,1)).all(dim=0))\n",
    "    edge_mask[ind[0]] = False\n",
    "    ind = torch.where((edge_index_p == mapping[[1,0]].view(-1,1)).all(dim=0))\n",
    "    edge_mask[ind[0]] = False\n",
    "    if args.drnl == True:\n",
    "        num_nodes = torch.max(edge_index_p)+1\n",
    "        z = drnl_node_labeling(edge_index_p[:,edge_mask], mapping[0],mapping[1],num_nodes)\n",
    "        data = Data(edge_index = edge_index_p, x= x_sub,z = z)\n",
    "    else:\n",
    "        data = Data(edge_index = edge_index_p, x= x_sub,z = 0)\n",
    "    data.edge_mask = edge_mask\n",
    "\n",
    "    #label = 1 if the candidate link (p_edge) is positive and label=0 otherwise\n",
    "    data.label = float(label)\n",
    "    return data\n",
    "\n",
    "\n",
    "def load_splitted_data(args):\n",
    "    par_dir = os.path.abspath(os.path.join(os.path.dirname(__file__),\"..\"))\n",
    "    data_name=args.data_name+'_split_'+args.data_split_num\n",
    "    if args.test_ratio==0.5:\n",
    "        data_dir = os.path.join(par_dir, 'data/splitted_0_5/{}.mat'.format(data_name))\n",
    "    else:\n",
    "        data_dir = os.path.join(par_dir, 'data/splitted/{}.mat'.format(data_name))\n",
    "    import scipy.io as sio\n",
    "    print('Load data from: '+data_dir)\n",
    "    net = sio.loadmat(data_dir)\n",
    "    data = Data()\n",
    "\n",
    "    data.train_pos = torch.from_numpy(np.int64(net['train_pos']))\n",
    "    data.train_neg = torch.from_numpy(np.int64(net['train_neg']))\n",
    "    data.test_pos = torch.from_numpy(np.int64(net['test_pos']))\n",
    "    data.test_neg = torch.from_numpy(np.int64(net['test_neg']))\n",
    "\n",
    "    n_pos= floor(args.val_ratio * len(data.train_pos)).int()\n",
    "    nlist=np.arange(len(data.train_pos))\n",
    "    np.random.shuffle(nlist)\n",
    "    val_pos_list=nlist[:n_pos]\n",
    "    train_pos_list=nlist[n_pos:]\n",
    "    data.val_pos=data.train_pos[val_pos_list]\n",
    "    data.train_pos=data.train_pos[train_pos_list]\n",
    "\n",
    "    n_neg = floor(args.val_ratio * len(data.train_neg)).int()\n",
    "    nlist=np.arange(len(data.train_neg))\n",
    "    np.random.shuffle(nlist)\n",
    "    val_neg_list=nlist[:n_neg]\n",
    "    train_neg_list=nlist[n_neg:]\n",
    "    data.val_neg=data.train_neg[val_neg_list]\n",
    "    data.train_neg=data.train_neg[train_neg_list]\n",
    "\n",
    "    data.val_pos = torch.transpose(data.val_pos,0,1)\n",
    "    data.val_neg = torch.transpose(data.val_neg,0,1)\n",
    "    data.train_pos = torch.transpose(data.train_pos,0,1)\n",
    "    data.train_neg = torch.transpose(data.train_neg,0,1)\n",
    "    data.test_pos = torch.transpose(data.test_pos,0,1)\n",
    "    data.test_neg = torch.transpose(data.test_neg,0,1)\n",
    "    num_nodes = max(torch.max(data.train_pos), torch.max(data.test_pos))+1\n",
    "    num_nodes=max(num_nodes,torch.max(data.val_pos)+1)\n",
    "    data.num_nodes = num_nodes\n",
    "\n",
    "    return data\n",
    "\n",
    "def load_unsplitted_data(args):\n",
    "    # read .mat format files\n",
    "    data_dir = os.path.join(par_dir, 'data/{}.mat'.format(args.data_name))\n",
    "    print('Load data from: '+ data_dir)\n",
    "    import scipy.io as sio\n",
    "    net = sio.loadmat(data_dir)\n",
    "    edge_index,_ = from_scipy_sparse_matrix(net['net'])\n",
    "    data = Data(edge_index=edge_index)\n",
    "    if is_undirected(data.edge_index) == False: #in case the dataset is directed\n",
    "        data.edge_index = to_undirected(data.edge_index)\n",
    "    data.num_nodes = torch.max(data.edge_index)+1\n",
    "    return data\n",
    "def load_Planetoid_data(args):\n",
    "    print('Using data: '+ args.data_name)\n",
    "    #dataset = Planetoid(root=par_dir+'/data/', name=args.data_name, transform=NormalizeFeatures())\n",
    "    dataset = Planetoid(root=par_dir+'/data/', name=args.data_name)\n",
    "    data = dataset[0]\n",
    "    data.num_nodes = torch.max(data.edge_index)+1\n",
    "    return data\n",
    "# def load_Planetoid_data(args):\n",
    "#     print('downloading data: '+ args.data_name)\n",
    "#     #dataset = Planetoid(root=par_dir+'/data/', name=args.data_name, transform=NormalizeFeatures())\n",
    "#     dataset = Planetoid(root=par_dir+'/data/', name=args.data_name)\n",
    "#     # Edited from https://github.com/tkipf/gae/blob/master/gae/input_data.py\n",
    "#     names = ['x', 'tx', 'allx', 'graph']\n",
    "#     objects = []\n",
    "#     for i in range(len(names)):\n",
    "#         with open(\"./data/{}/raw/ind.{}.{}\".format(args.data_name,args.data_name, names[i]), 'rb') as f:\n",
    "#             if sys.version_info > (3, 0):\n",
    "#                 objects.append(pkl.load(f, encoding='latin1'))\n",
    "#             else:\n",
    "#                 objects.append(pkl.load(f))\n",
    "#     x, tx, allx, graph = tuple(objects)\n",
    "#     filename=\"./data/{}/raw/ind.{}.test.index\".format(args.data_name,args.data_name)\n",
    "#     index = []\n",
    "#     for line in open(filename):\n",
    "#         index.append(int(line.strip()))\n",
    "#     test_idx_reorder = index\n",
    "#     test_idx_range = np.sort(test_idx_reorder)\n",
    "#     if args.data_name == 'citeseer':\n",
    "#         # Fix citeseer dataset (there are some isolated nodes in the graph)\n",
    "#         # Find isolated nodes, add them as zero-vecs into the right position\n",
    "#         test_idx_range_full = range(min(test_idx_reorder), max(test_idx_reorder)+1)\n",
    "#         tx_extended = sp.lil_matrix((len(test_idx_range_full), x.shape[1]))\n",
    "#         tx_extended[test_idx_range-min(test_idx_range), :] = tx\n",
    "#         tx = tx_extended\n",
    "#     features = sp.vstack((allx, tx)).tolil()\n",
    "#     features[test_idx_reorder, :] = features[test_idx_range, :]\n",
    "#     features=torch.tensor(sp.coo_matrix.todense(features)).float()\n",
    "#     adj = nx.adjacency_matrix(nx.from_dict_of_lists(graph))\n",
    "#     edge_index=from_scipy_sparse_matrix(adj)[0]\n",
    "#     data=Data(edge_index=edge_index,x=features)\n",
    "#     data.num_nodes = torch.max(data.edge_index)+1\n",
    "\n",
    "#     return data\n",
    "\n",
    "\n",
    "def set_init_attribute_representation(data,args):\n",
    "    #Construct data_observed and compute its node attributes & representation\n",
    "    edge_index = torch.cat((data.train_pos,data.train_pos[[1,0],:]),dim=1)\n",
    "    if args.observe_val_and_injection == False:\n",
    "        data_observed = Data(edge_index=edge_index)\n",
    "    else:\n",
    "        edge_index=torch.cat((edge_index,data.val_pos,data.val_pos[[1,0],:]),dim=1)\n",
    "        data_observed = Data(edge_index=edge_index)\n",
    "    data_observed.num_nodes = data.num_nodes\n",
    "    if args.observe_val_and_injection == False:\n",
    "        edge_index_observed = data_observed.edge_index\n",
    "    else: \n",
    "        #use the injection trick and add val data in observed graph \n",
    "        edge_index_observed = torch.cat((data_observed.edge_index,\\\n",
    "            data.train_neg,data.train_neg[[1,0],:],data.val_neg,data.val_neg[[1,0],:]),dim=1)\n",
    "    #generate the initial node attribute if there isn't any\n",
    "    if data.x == None:\n",
    "        if args.init_attribute =='n2v':\n",
    "            from node2vec import CalN2V\n",
    "            x = CalN2V(edge_index_observed,args)\n",
    "        if args.init_attribute =='one_hot':\n",
    "            x = F.one_hot(torch.arange(data.num_nodes), num_classes=data.num_nodes)\n",
    "            x = x.float()\n",
    "        if args.init_attribute =='spc':\n",
    "            from SPC import spc\n",
    "            x = spc(edge_index_observed,args)\n",
    "            x = x.float()\n",
    "        if args.init_attribute =='ones':\n",
    "            x = torch.ones(data.num_nodes,args.embedding_dim)\n",
    "            x = x.float()\n",
    "        if args.init_attribute =='zeros':\n",
    "            x = torch.zeros(data.num_nodes,args.embedding_dim)\n",
    "            x = x.float()\n",
    "    else:\n",
    "        x = data.x\n",
    "    #generate the initial node representation using unsupervised models\n",
    "    if args.init_representation != None:\n",
    "        val_and_test=[data.test_pos,data.test_neg,data.val_pos,data.val_neg]\n",
    "        num_nodes,_=x.shape\n",
    "        #add self-loop for the last node to aviod losing node if the last node dosen't have a link.\n",
    "        if (num_nodes-1) in edge_index_observed:\n",
    "            edge_index_observed=edge_index_observed.clone().detach()\n",
    "        else:\n",
    "            edge_index_observed=torch.cat((edge_index_observed.clone().detach(),torch.tensor([[num_nodes-1],[num_nodes-1]])),dim=1)\n",
    "        if args.init_representation == 'gic':\n",
    "            args.par_dir = os.path.abspath(os.path.join(os.path.dirname(__file__),\"..\"))\n",
    "            sys.path.append('%s/software/GIC/' % args.par_dir)\n",
    "            from GICEmbs import CalGIC\n",
    "            data_observed.x, auc, ap = CalGIC(edge_index_observed, x, args.data_name, val_and_test,args)\n",
    "\n",
    "        if args.init_representation == 'vgae':\n",
    "            from vgae import CalVGAE\n",
    "            data_observed.x, auc, ap = CalVGAE(edge_index_observed, x, val_and_test, args)\n",
    "        if args.init_representation == 'svgae':\n",
    "            from svgae import CalSVGAE\n",
    "            data_observed.x, auc, ap = CalSVGAE(edge_index_observed, x, val_and_test, args)\n",
    "        if args.init_representation == 'argva':\n",
    "            from argva import CalARGVA\n",
    "            data_observed.x, auc, ap = CalARGVA(edge_index_observed, x, val_and_test, args)\n",
    "        feature_results=[auc,ap]\n",
    "    else:\n",
    "        data_observed.x = x\n",
    "        feature_results=None\n",
    "\n",
    "    return data_observed,feature_results\n",
    "\n",
    "\n",
    "def prepare_data(args):\n",
    "    #load data from .mat or download from Planetoid dataset.\n",
    "    \n",
    "    if args.data_name in ('cora', 'citeseer', 'pubmed'):\n",
    "        data = load_Planetoid_data(args)\n",
    "        data = split_edges(data,args)\n",
    "    else:\n",
    "        if args.use_splitted == True: #use splitted train/val/test\n",
    "            data = load_splitted_data(args)\n",
    "        else:\n",
    "            data = load_unsplitted_data(args)\n",
    "            data = split_edges(data,args)\n",
    "    \n",
    "    \n",
    "\n",
    "    set_random_seed(args.seed)\n",
    "    data_observed,feature_results= set_init_attribute_representation(data,args)\n",
    "\n",
    "    #Construct train, val and test data loader.\n",
    "    set_random_seed(args.seed)\n",
    "    train_graphs = []\n",
    "    val_graphs = []\n",
    "    test_graphs = []\n",
    "    for i in range(data.train_pos.size(1)):\n",
    "        train_graphs.append(minus_edge(data_observed,1,data.train_pos[:,i],args))\n",
    "\n",
    "    for i in range(data.train_neg.size(1)):\n",
    "        train_graphs.append(plus_edge(data_observed,0,data.train_neg[:,i],args))\n",
    "\n",
    "    for i in range(data.test_pos.size(1)):\n",
    "        test_graphs.append(plus_edge(data_observed,1,data.test_pos[:,i],args))\n",
    "\n",
    "    for i in range(data.test_neg.size(1)):\n",
    "        test_graphs.append(plus_edge(data_observed,0,data.test_neg[:,i],args))   \n",
    "    if args.observe_val_and_injection == False:\n",
    "        for i in range(data.val_pos.size(1)):\n",
    "            val_graphs.append(plus_edge(data_observed,1,data.val_pos[:,i],args))\n",
    "\n",
    "        for i in range(data.val_neg.size(1)):\n",
    "            val_graphs.append(plus_edge(data_observed,0,data.val_neg[:,i],args))\n",
    "    else:\n",
    "        for i in range(data.val_pos.size(1)):\n",
    "            val_graphs.append(minus_edge(data_observed,1,data.val_pos[:,i],args))\n",
    "\n",
    "        for i in range(data.val_neg.size(1)):\n",
    "            val_graphs.append(plus_edge(data_observed,0,data.val_neg[:,i],args))\n",
    "\n",
    "\n",
    "    \n",
    "    print('Train_link:', str(len(train_graphs)),' Val_link:',str(len(val_graphs)),' Test_link:',str(len(test_graphs)))\n",
    "\n",
    "    train_loader = DataLoader(train_graphs,batch_size=args.batch_size,shuffle=True)\n",
    "    val_loader = DataLoader(val_graphs,batch_size=args.batch_size,shuffle=True)\n",
    "    test_loader = DataLoader(test_graphs,batch_size=args.batch_size,shuffle=False)\n",
    "\n",
    "    return train_loader, val_loader, test_loader,feature_results\n",
    "\n",
    "\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation ratio: 0.18463810930576072\n",
      "Test ratio: 0.051698670605613\n",
      "Learning rate: 0.0005\n"
     ]
    }
   ],
   "source": [
    "# Link prediction message passing model settings\n",
    "number_of_heads_in_attention_link_encoder = 2\n",
    "walk_length = 2 # Number of learning iterations based on messages passing, called \"walk length\" (#TODO referenciar paper)\n",
    "use_mse = False\n",
    "practical_neg_sample = False\n",
    "\n",
    "# Copy validation and test ratios from experiments performed using Spektral\n",
    "val_ratio = np.sum(dataset.mask_va)/number_of_nodes\n",
    "test_ratio = np.sum(dataset.mask_tr)/number_of_nodes\n",
    "\n",
    "class MyDict(dict):\n",
    "    pass\n",
    "\n",
    "args = MyDict()\n",
    "args.data_name = 'cora'\n",
    "args.seed = 1\n",
    "args.use_splitted = False\n",
    "args.practical_neg_sample = True\n",
    "args.observe_val_and_injection = False\n",
    "args.init_attribute=None\n",
    "args.lr = 0.0005 # learning rate\n",
    "args.val_ratio = 0.2\n",
    "args.test_ratio = 0.2\n",
    "args.init_representation = None\n",
    "args.num_hops = 3\n",
    "args.max_nodes_per_hop = 100\n",
    "args.drnl = False\n",
    "args.batch_size = 32\n",
    "args.weight_decay = 0\n",
    "args.epoch_num = 10\n",
    "\n",
    "\n",
    "# Get device definition, by default cuda\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Instance LinkPred model\n",
    "model = LinkPred(\n",
    "    in_channels=number_of_node_features, # Number of nodes used as input data\n",
    "    hidden_channels=number_of_labels, # Number of labels used as output data\n",
    "    heads=number_of_heads_in_attention_link_encoder, # Number of attention heads\n",
    "    walk_len=walk_length, \n",
    "    # drnl = args.drnl,\n",
    "    # z_max = z_max, \n",
    "    MSE=use_mse\n",
    ").to(device)\n",
    "\n",
    "# Select optimizer and loss function\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)\n",
    "criterion = torch.nn.MSELoss(reduction='mean')\n",
    "\n",
    "# Prompt settings to notebook\n",
    "print(f'Validation ratio: {val_ratio}')\n",
    "print(f'Test ratio: {test_ratio}')\n",
    "print(f'Learning rate: {args.lr}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using data: cora\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.x\n",
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.tx\n",
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.allx\n",
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.y\n",
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.ty\n",
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.ally\n",
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.graph\n",
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.test.index\n",
      "Processing...\n",
      "Done!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train_link: 6336  Val_link: 2110  Test_link: 2110\n"
     ]
    }
   ],
   "source": [
    "# Prepare data\n",
    "train_loader, val_loader, test_loader, feature_results = prepare_data(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "from tqdm import tqdm\n",
    "\n",
    "def train(loader,epoch):\n",
    "    model.train()\n",
    "    loss_epoch=0\n",
    "    for data in tqdm(loader,desc=\"train\"):  # Iterate in batches over the training dataset.\n",
    "        data = data.to(device)\n",
    "        label= data.label\n",
    "        out = model(data.x, data.edge_index, data.edge_mask, data.batch, data.z)\n",
    "        torch.cuda.empty_cache()\n",
    "        loss = criterion(out.view(-1), label)  \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()  \n",
    "        optimizer.step()\n",
    "        loss_epoch=loss_epoch+loss.item()\n",
    "    return loss_epoch/len(loader)\n",
    "\n",
    "\n",
    "def test(loader,data_type='test'):\n",
    "    model.eval()\n",
    "    scores = torch.tensor([])\n",
    "    labels = torch.tensor([])\n",
    "    loss_total=0\n",
    "    with torch.no_grad():\n",
    "        for data in tqdm(loader,desc='test:'+data_type):  # Iterate in batches over the training/test dataset.\n",
    "            data = data.to(device)\n",
    "            out = model(data.x, data.edge_index, data.edge_mask, data.batch, data.z)\n",
    "            loss = criterion(out.view(-1), data.label)\n",
    "            out = out.cpu().clone().detach()\n",
    "            scores = torch.cat((scores,out),dim = 0)\n",
    "            labels = torch.cat((labels,data.label.view(-1,1).cpu().clone().detach()),dim = 0)\n",
    "        scores = scores.cpu().clone().detach().numpy()\n",
    "        labels = labels.cpu().clone().detach().numpy()\n",
    "        loss_total=loss_total+loss.item()\n",
    "        return roc_auc_score(labels, scores), average_precision_score(labels, scores),loss_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|██████████| 198/198 [01:03<00:00,  3.11it/s]\n",
      "test:val: 100%|██████████| 66/66 [00:10<00:00,  6.21it/s]\n",
      "test:test: 100%|██████████| 66/66 [00:08<00:00,  7.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001\t Loss: 0.2971\n",
      "Validation accuracy: 0.8410\tValidation loss: 0.2074\n",
      "Test accuracy: 0.8268\tTest loss: 0.1109\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train:  20%|██        | 40/198 [00:13<00:51,  3.07it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/ql/3w89m27d7mn4c5fdsnrhrrp00000gn/T/ipykernel_8137/2695061273.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Iterate each epoch to train the model. Send performance metrics to TensorBoard\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepoch_num\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mepoch_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_ap\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'val'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_ap\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'test'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/ql/3w89m27d7mn4c5fdsnrhrrp00000gn/T/ipykernel_8137/3993885772.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(loader, epoch)\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mloss_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdesc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"train\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Iterate in batches over the training dataset.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/virtualenvs/uoc-tfg-gnn/lib/python3.9/site-packages/tqdm/std.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1178\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1180\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1181\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1182\u001b[0m                 \u001b[0;31m# Update and possibly print the progressbar.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/virtualenvs/uoc-tfg-gnn/lib/python3.9/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    519\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    520\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 521\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    522\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/virtualenvs/uoc-tfg-gnn/lib/python3.9/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    559\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    560\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 561\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    562\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    563\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/virtualenvs/uoc-tfg-gnn/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/virtualenvs/uoc-tfg-gnn/lib/python3.9/site-packages/torch_geometric/loader/dataloader.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/virtualenvs/uoc-tfg-gnn/lib/python3.9/site-packages/torch_geometric/loader/dataloader.py\u001b[0m in \u001b[0;36mcollate\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0melem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mData\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mHeteroData\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m             return Batch.from_data_list(batch, self.follow_batch,\n\u001b[0m\u001b[1;32m     20\u001b[0m                                         self.exclude_keys)\n\u001b[1;32m     21\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/virtualenvs/uoc-tfg-gnn/lib/python3.9/site-packages/torch_geometric/data/batch.py\u001b[0m in \u001b[0;36mfrom_data_list\u001b[0;34m(cls, data_list, follow_batch, exclude_keys)\u001b[0m\n\u001b[1;32m     67\u001b[0m         Will exclude any keys given in :obj:`exclude_keys`.\"\"\"\n\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m         batch, slice_dict, inc_dict = collate(\n\u001b[0m\u001b[1;32m     70\u001b[0m             \u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m             \u001b[0mdata_list\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/virtualenvs/uoc-tfg-gnn/lib/python3.9/site-packages/torch_geometric/data/collate.py\u001b[0m in \u001b[0;36mcollate\u001b[0;34m(cls, data_list, increment, add_batch, follow_batch, exclude_keys)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcls\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mdata_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_base_cls\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Dynamic inheritance.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/virtualenvs/uoc-tfg-gnn/lib/python3.9/site-packages/torch_geometric/data/batch.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m     39\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDynamicInheritance\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_cls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/virtualenvs/uoc-tfg-gnn/lib/python3.9/site-packages/torch_geometric/data/data.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, x, edge_index, edge_attr, y, pos, **kwargs)\u001b[0m\n\u001b[1;32m    322\u001b[0m         \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0moptional\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAdditional\u001b[0m \u001b[0mattributes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m     \"\"\"\n\u001b[0;32m--> 324\u001b[0;31m     def __init__(self, x: OptTensor = None, edge_index: OptTensor = None,\n\u001b[0m\u001b[1;32m    325\u001b[0m                  \u001b[0medge_attr\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptTensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptTensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    326\u001b[0m                  pos: OptTensor = None, **kwargs):\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Prepare TensorBoard to receive data from this training\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "tb = SummaryWriter(log_dir=get_run_logdir('mpnn_walkpooling'))\n",
    "\n",
    "# Iterate each epoch to train the model. Send performance metrics to TensorBoard\n",
    "for epoch in range(1, args.epoch_num + 1):\n",
    "    epoch_loss = train(train_loader, epoch)\n",
    "    _, val_ap, val_loss = test(val_loader, data_type='val')\n",
    "    _, test_ap, test_loss = test(test_loader, data_type='test')\n",
    "\n",
    "    # Log performance metrics into TensorBoard\n",
    "    tb.add_scalar('epoch_loss', epoch_loss, epoch) # Log epoch loss\n",
    "    # tb.add_scalar('epoch_acc', val_ap, epoch) # Log epoch accuracy\n",
    "    tb.add_scalar('validation_loss', val_loss, epoch) # Log epoch validation loss\n",
    "    tb.add_scalar('validation_acc', val_ap, epoch) # Log epoch validation accuracy\n",
    "    tb.add_scalar('test_loss', test_ap, epoch) # Log test loss\n",
    "    tb.add_scalar('test_acc', test_loss, epoch) # Log test accuracy\n",
    "\n",
    "    # Prompt to notebook\n",
    "    print(f'Epoch: {epoch:03d}\\t Loss: {epoch_loss:.4f}\\nValidation accuracy: {val_ap:.4f}\\tValidation loss: {val_loss:.4f}\\nTest accuracy: {test_ap:.4f}\\tTest loss: {test_loss:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a0080289cbe72122e4cc1d387721a77267b793dcb0222027115d0fe70e838b1b"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('uoc-tfg-gnn': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
